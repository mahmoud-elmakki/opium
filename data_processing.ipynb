{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38bcc1b-d0d6-41b5-8f62-8764839b9bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "\n",
    "\n",
    "\n",
    "def estimate_poisson_rate_bias(y, time_delta):\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        bias_hat = torch.log(torch.mean(y, dim=[0, 1]) / time_delta + 1e-12)\n",
    "\n",
    "    elif isinstance(y, torch.utils.data.DataLoader):\n",
    "        n_batch = 0\n",
    "\n",
    "        for dx, y_mb in enumerate(y):\n",
    "            if dx == 0:\n",
    "                full_batch_mean = torch.zeros(y_mb[0].shape[-1], device=y_mb[0].device)\n",
    "\n",
    "            full_batch_mean += torch.mean(y_mb[0], dim=[0, 1])\n",
    "            n_batch += 1\n",
    "\n",
    "        full_batch_mean /= n_batch\n",
    "        bias_hat = torch.log(full_batch_mean / time_delta + 1e-12)\n",
    "    else:\n",
    "        raise TypeError('pass in tensor or dataloader')\n",
    "\n",
    "    return bias_hat\n",
    "\n",
    "\n",
    "def tuple_mapping(tuples_list):\n",
    "    count = 0\n",
    "    tuple_dict = {}\n",
    "    id_type_dict = {}\n",
    "\n",
    "    proper_tuples_list = []\n",
    "\n",
    "    for tup in tuples_list:\n",
    "        proper_tuple = (int(tup[0]), int(tup[1]))\n",
    "        proper_tuples_list.append(proper_tuple)\n",
    "\n",
    "        if proper_tuple not in tuple_dict:\n",
    "            tuple_dict[proper_tuple] = count\n",
    "            id_type_dict[count] = proper_tuple\n",
    "            count += 1\n",
    "\n",
    "    mapped_list = [tuple_dict[tup] for tup in proper_tuples_list]\n",
    "    return mapped_list, id_type_dict\n",
    "\n",
    "\n",
    "def get_expanded_dataset(dataset, binsize, start, end):\n",
    "    lag = 100\n",
    "\n",
    "    go_cue = []\n",
    "    spikes = []\n",
    "    velocity = []\n",
    "    trial_type = []\n",
    "    trial_version = []\n",
    "    unique_rt = dataset.trial_info.set_index(['rt']).index.unique().tolist()\n",
    "\n",
    "    for dx, rt in enumerate(unique_rt):\n",
    "        if dx == 0:\n",
    "            continue\n",
    "        if rt//binsize >= 49:\n",
    "            continue\n",
    "\n",
    "        mask = np.all(dataset.trial_info[['rt']] == rt, axis=1)\n",
    "\n",
    "        trial_data = dataset.make_trial_data(align_field='move_onset_time',\n",
    "                                             align_range=(start, end),\n",
    "                                             ignored_trials=(~mask))\n",
    "        lagged_trial_data = dataset.make_trial_data(align_field='move_onset_time',\n",
    "                                                    align_range=(start+lag, end+lag),\n",
    "                                                    ignored_trials=(~mask))\n",
    "\n",
    "        velocity_trial = lagged_trial_data.hand_vel.to_numpy()\n",
    "\n",
    "        trial_length = int(end - start) // binsize\n",
    "        n_trials = dataset.trial_info[mask].shape[0]\n",
    "\n",
    "        spikes_per_trial = torch.tensor(trial_data.spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                        dtype=torch.float32)\n",
    "        heldout_spikes_per_trial = torch.tensor(trial_data.heldout_spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                                dtype=torch.float32)\n",
    "\n",
    "        all_spikes_per_trial = np.concatenate([spikes_per_trial, heldout_spikes_per_trial], axis=-1)\n",
    "        velocity_per_trial = velocity_trial.reshape(n_trials, trial_length, -1)\n",
    "        velocity_per_trial = np.nan_to_num(velocity_per_trial, nan=0.0)\n",
    "\n",
    "        go_cue.append(torch.tensor(-start//binsize - dataset.trial_info[mask].rt.values//binsize))\n",
    "        trial_version.append(torch.tensor(dataset.trial_info[mask].trial_version.values))\n",
    "        trial_type.append(torch.tensor(dataset.trial_info[mask].trial_type.values))\n",
    "        spikes.append(torch.tensor(all_spikes_per_trial))\n",
    "        velocity.append(torch.tensor(velocity_per_trial))\n",
    "\n",
    "    go_cue = torch.cat(go_cue)\n",
    "    trial_version = torch.cat(trial_version)\n",
    "    trial_type = torch.cat(trial_type)\n",
    "    velocity = torch.cat(velocity, dim=0)\n",
    "    spikes = torch.cat(spikes, dim=0)\n",
    "\n",
    "    trial_ids, id_type_dict = tuple_mapping(torch.cat([trial_version.unsqueeze(-1), trial_type.unsqueeze(-1)], dim=-1))\n",
    "    go_signal = torch.zeros((spikes.shape[0], spikes.shape[1], 1))\n",
    "    trial_ids = torch.tensor(trial_ids)\n",
    "\n",
    "    mv_onset = torch.zeros((spikes.shape[0], spikes.shape[1], 1))\n",
    "    mv_onset[:, -start//binsize] = 1\n",
    "\n",
    "    for i in range(go_signal.shape[0]):\n",
    "        go_signal[i, go_cue.type(torch.int)[i]] = 1\n",
    "\n",
    "    trial_dx = torch.randperm(spikes.shape[0])\n",
    "    return spikes[trial_dx], velocity[trial_dx], go_signal[trial_dx], mv_onset, trial_ids[trial_dx], id_type_dict\n",
    "\n",
    "\n",
    "def get_expanded_dataset_target_aligned(dataset, binsize, start, end, lag=100, mv_signal_lag=10):\n",
    "    trial_length = int(end - start) // binsize\n",
    "    mask = np.all(dataset.trial_info[['trial_id']] >= 574, axis=1)\n",
    "    n_trials = mask[mask == True].shape[0]\n",
    "\n",
    "    rt = torch.tensor(dataset.trial_info['rt'][mask].values)\n",
    "    delay = torch.tensor(dataset.trial_info['delay'][mask].values)\n",
    "    trial_data = dataset.make_trial_data(align_field='target_on_time',\n",
    "                                         align_range=(start, end),\n",
    "                                         ignored_trials=~mask)\n",
    "    lagged_trial_data = dataset.make_trial_data(align_field='target_on_time',\n",
    "                                                align_range=(start + lag, end + lag),\n",
    "                                                ignored_trials=~mask)\n",
    "\n",
    "    velocity = lagged_trial_data.hand_vel.to_numpy()\n",
    "    velocity = velocity.reshape(n_trials, trial_length, -1)\n",
    "    velocity = torch.tensor(np.nan_to_num(velocity, nan=0.0))\n",
    "\n",
    "    heldin_spikes = torch.tensor(trial_data.spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                    dtype=torch.float32)\n",
    "    heldout_spikes = torch.tensor(trial_data.heldout_spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                            dtype=torch.float32)\n",
    "    spikes = torch.cat([heldin_spikes, heldout_spikes], dim=-1)\n",
    "\n",
    "    trial_type = torch.tensor(dataset.trial_info[mask].trial_type.values)\n",
    "    trial_version = torch.tensor(dataset.trial_info[mask].trial_version.values)\n",
    "    trial_ids, id_type_dict = tuple_mapping(torch.cat([trial_version.unsqueeze(-1), trial_type.unsqueeze(-1)], dim=-1))\n",
    "    trial_ids = torch.tensor(trial_ids)\n",
    "\n",
    "    go_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "    mv_onset_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "\n",
    "    for trial_dx, (rt_trial, delay_trial) in enumerate(zip(rt, delay)):\n",
    "        rt_bins = (rt_trial//20).type(torch.int)\n",
    "        delay_bins = (delay_trial//20).type(torch.int)\n",
    "\n",
    "        go_signal[trial_dx, delay_bins-(start//binsize)] = 1.\n",
    "\n",
    "        if delay_bins+rt_bins < end//binsize:\n",
    "            mv_onset_signal[trial_dx, delay_bins+rt_bins-(start//20)-mv_signal_lag] = 1.\n",
    "\n",
    "    trial_dx = torch.randperm(spikes.shape[0])\n",
    "    return (spikes[trial_dx], velocity[trial_dx], go_signal[trial_dx], mv_onset_signal[trial_dx], trial_ids[trial_dx],\n",
    "            id_type_dict, rt[trial_dx], delay[trial_dx])\n",
    "\n",
    "\n",
    "def get_expanded_dataset_go_aligned(dataset, binsize, start, end, lag=100, mv_signal_lag=10):\n",
    "    trial_length = int(end - start) // binsize\n",
    "    mask = np.all(dataset.trial_info[['trial_id']] >= 574, axis=1)\n",
    "    n_trials = mask[mask == True].shape[0]\n",
    "\n",
    "    rt = torch.tensor(dataset.trial_info['rt'][mask].values)\n",
    "    delay = torch.tensor(dataset.trial_info['delay'][mask].values)\n",
    "    trial_data = dataset.make_trial_data(align_field='go_cue_time',\n",
    "                                         align_range=(start, end),\n",
    "                                         ignored_trials=~mask)\n",
    "    lagged_trial_data = dataset.make_trial_data(align_field='go_cue_time',\n",
    "                                                align_range=(start + lag, end + lag),\n",
    "                                                ignored_trials=~mask)\n",
    "\n",
    "    velocity = lagged_trial_data.hand_vel.to_numpy()\n",
    "    velocity = velocity.reshape(n_trials, trial_length, -1)\n",
    "    velocity = torch.tensor(np.nan_to_num(velocity, nan=0.0))\n",
    "\n",
    "    heldin_spikes = torch.tensor(trial_data.spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                    dtype=torch.float32)\n",
    "    heldout_spikes = torch.tensor(trial_data.heldout_spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                            dtype=torch.float32)\n",
    "    spikes = torch.cat([heldin_spikes, heldout_spikes], dim=-1)\n",
    "\n",
    "    trial_type = torch.tensor(dataset.trial_info[mask].trial_type.values)\n",
    "    trial_version = torch.tensor(dataset.trial_info[mask].trial_version.values)\n",
    "    trial_ids, id_type_dict = tuple_mapping(torch.cat([trial_version.unsqueeze(-1), trial_type.unsqueeze(-1)], dim=-1))\n",
    "    trial_ids = torch.tensor(trial_ids)\n",
    "\n",
    "    go_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "    mv_onset_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "\n",
    "    for trial_dx, (rt_trial, delay_trial) in enumerate(zip(rt, delay)):\n",
    "        rt_bins = (rt_trial//binsize).type(torch.int)\n",
    "        delay_bins = (delay_trial//binsize).type(torch.int)\n",
    "\n",
    "        go_signal[trial_dx, -start//binsize] = 1.\n",
    "\n",
    "        if rt_bins < end//binsize:\n",
    "            mv_onset_signal[trial_dx, -(start//binsize)+rt_bins-mv_signal_lag] = 1.\n",
    "\n",
    "    trial_dx = torch.randperm(spikes.shape[0])\n",
    "    return (spikes[trial_dx], velocity[trial_dx], go_signal[trial_dx], mv_onset_signal[trial_dx],\n",
    "            trial_ids[trial_dx], id_type_dict, rt[trial_dx], delay[trial_dx])\n",
    "\n",
    "\n",
    "def get_expanded_dataset_move_onset_aligned(dataset, binsize, start, end, lag=100, mv_signal_lag=10):\n",
    "    trial_length = int(end - start) // binsize\n",
    "    mask = np.all(dataset.trial_info[['trial_id']] >= 574, axis=1)\n",
    "    n_trials = mask[mask == True].shape[0]\n",
    "\n",
    "    rt = torch.tensor(dataset.trial_info['rt'][mask].values)\n",
    "    delay = torch.tensor(dataset.trial_info['delay'][mask].values)\n",
    "    trial_type = torch.tensor(dataset.trial_info['trial_type'][mask].values)\n",
    "\n",
    "    trial_data = dataset.make_trial_data(align_field='move_onset_time',\n",
    "                                         align_range=(start, end),\n",
    "                                         ignored_trials=~mask)\n",
    "    lagged_trial_data = dataset.make_trial_data(align_field='move_onset_time',\n",
    "                                                align_range=(start + lag, end + lag),\n",
    "                                                ignored_trials=~mask)\n",
    "\n",
    "    velocity = lagged_trial_data.hand_vel.to_numpy()\n",
    "    velocity = velocity.reshape(n_trials, trial_length, -1)\n",
    "    velocity = torch.tensor(np.nan_to_num(velocity, nan=0.0))\n",
    "\n",
    "    heldin_spikes = torch.tensor(trial_data.spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                    dtype=torch.float32)\n",
    "    heldout_spikes = torch.tensor(trial_data.heldout_spikes.values.reshape(n_trials, trial_length, -1),\n",
    "                                            dtype=torch.float32)\n",
    "    spikes = torch.cat([heldin_spikes, heldout_spikes], dim=-1)\n",
    "\n",
    "    trial_type = torch.tensor(dataset.trial_info[mask].trial_type.values)\n",
    "    trial_version = torch.tensor(dataset.trial_info[mask].trial_version.values)\n",
    "    trial_ids, id_type_dict = tuple_mapping(torch.cat([trial_version.unsqueeze(-1), trial_type.unsqueeze(-1)], dim=-1))\n",
    "    trial_ids = torch.tensor(trial_ids)\n",
    "\n",
    "    go_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "    mv_onset_signal = torch.zeros((n_trials, trial_length, 1))\n",
    "\n",
    "    for trial_dx, (rt_trial, delay_trial) in enumerate(zip(rt, delay, trial_type)):\n",
    "        rt_bins = (rt_trial//20).type(torch.int)\n",
    "\n",
    "        if -(start//binsize) - rt_bins >= 0:\n",
    "            go_signal[trial_dx, -(start//binsize) - rt_bins] = 1.\n",
    "\n",
    "        if -(start//binsize) - mv_signal_lag > 0:\n",
    "            mv_onset_signal[trial_dx, -(start//binsize) - mv_signal_lag] = 1.\n",
    "\n",
    "    trial_dx = torch.randperm(spikes.shape[0])\n",
    "    return (spikes[trial_dx], velocity[trial_dx], go_signal[trial_dx], mv_onset_signal[trial_dx], trial_ids[trial_dx],\n",
    "            id_type_dict, rt[trial_dx], delay[trial_dx])\n",
    "\n",
    "\n",
    "def main():\n",
    "    lag = 100\n",
    "    # + 500, - 1000  for target_on\n",
    "    # + 460, - 240 for movement\n",
    "    end = 500\n",
    "    start = -1000\n",
    "\n",
    "    binsize = 20\n",
    "    save_path = '/Users/mahmoud/data/'\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    align_at = 'move_onset_time'\n",
    "    #align_at = 'move_on_time'\n",
    "    #align_at = 'go_cue_time'\n",
    "    datapath = '/Users/mahmoud/data/NWB/000128/sub-Jenkins/sub-Jenkins_ses-full_desc-train_behavior+ecephys.nwb'\n",
    "    dataset = NWBDataset(datapath)\n",
    "    dataset.resample(binsize)\n",
    "\n",
    "    # for align_at in ['move_onset_time', 'target_on_time', 'go_cue_time']:\n",
    "    for align_at in ['move_onset_time', 'target_on_mo_align']:\n",
    "        if align_at == 'target_on_time':\n",
    "            spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay = get_expanded_dataset_target_aligned(dataset,\n",
    "                                                                                                                 binsize,                                                                                                    -200,1200)\n",
    "        elif align_at == 'go_cue_time':\n",
    "            spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay = get_expanded_dataset_go_aligned(dataset, binsize,\n",
    "                                                                                                         -200, 1100)\n",
    "        elif align_at == 'target_on_mo_align':\n",
    "            spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay = get_expanded_dataset_move_onset_aligned(dataset, binsize,\n",
    "                                                                                                         -800, 500)\n",
    "        else:\n",
    "            spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay = get_expanded_dataset_move_onset_aligned(dataset, binsize,                                                                                          -240, 460)\n",
    "\n",
    "        train_test_split = lambda x: (x[n_test_trials:], x[:n_test_trials//2], x[n_test_trials//2:n_test_trials])\n",
    "        train_data = {}\n",
    "        valid_data = {}\n",
    "        test_data = {}\n",
    "\n",
    "        train_data['y_obs'], test_data['y_obs'], valid_data['y_obs'] = train_test_split(spikes)\n",
    "        train_data['y_enc'], test_data['y_enc'], valid_data['y_enc'] = train_test_split(spikes)\n",
    "        train_data['y_hld'], test_data['y_hld'], valid_data['y_hld'] = train_test_split(spikes)\n",
    "        train_data['velocity'], test_data['velocity'], valid_data['velocity'] = train_test_split(velocity)\n",
    "        train_data['go_input'], test_data['go_input'], valid_data['go_input'] = train_test_split(go_signal)\n",
    "        train_data['mv_input'], test_data['mv_input'], valid_data['mv_input'] = train_test_split(mv_onset)\n",
    "        train_data['trial_id'], test_data['trial_id'], valid_data['trial_id'] = train_test_split(trial_ids)\n",
    "        train_data['delay'], test_data['delay'], valid_data['delay'] = train_test_split(delay)\n",
    "        train_data['rt'], test_data['rt'], valid_data['rt'] = train_test_split(rt)\n",
    "\n",
    "        test_data['position'] = torch.cumsum(test_data['velocity'], dim=1)\n",
    "        train_data['position'] = torch.cumsum(train_data['velocity'], dim=1)\n",
    "        valid_data['position'] = torch.cumsum(valid_data['velocity'], dim=1)\n",
    "\n",
    "        test_data['input'] = torch.cat([test_data['mv_input'], test_data['go_input']], dim=-1)\n",
    "        train_data['input'] = torch.cat([train_data['mv_input'], train_data['go_input']], dim=-1)\n",
    "        valid_data['input'] = torch.cat([valid_data['mv_input'], valid_data['go_input']], dim=-1)\n",
    "\n",
    "        test_data['n_time_bins_enc'] = spikes.shape[1]\n",
    "        train_data['n_time_bins_enc'] = spikes.shape[1]\n",
    "        valid_data['n_time_bins_enc'] = spikes.shape[1]\n",
    "\n",
    "        torch.save(train_data, save_path + f'align_at_{align_at}/data_train_{binsize}ms_{align_at}.pt')\n",
    "        torch.save(test_data, save_path + f'align_at_{align_at}/data_test_{binsize}ms_{align_at}.pt')\n",
    "        torch.save(valid_data, save_path + f'align_at_{align_at}/data_valid_{binsize}ms_{align_at}.pt')\n",
    "        torch.save(id_type_dict, save_path + f'id_type_map.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fed2c53-0af1-46f0-8f6f-f7577684ee52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/opium/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/opt/anaconda3/envs/opium/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/opt/anaconda3/envs/opium/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 287\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay \u001b[38;5;241m=\u001b[39m get_expanded_dataset_move_onset_aligned(dataset, binsize,\n\u001b[1;32m    285\u001b[0m                                                                                                  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     spikes, velocity, go_signal, mv_onset, trial_ids, id_type_dict, rt, delay \u001b[38;5;241m=\u001b[39m \u001b[43mget_expanded_dataset_move_onset_aligned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                                                                          \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m460\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m train_test_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (x[n_test_trials:], x[:n_test_trials\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m], x[n_test_trials\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:n_test_trials])\n\u001b[1;32m    290\u001b[0m train_data \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[27], line 243\u001b[0m, in \u001b[0;36mget_expanded_dataset_move_onset_aligned\u001b[0;34m(dataset, binsize, start, end, lag, mv_signal_lag)\u001b[0m\n\u001b[1;32m    240\u001b[0m go_signal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((n_trials, trial_length, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    241\u001b[0m mv_onset_signal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((n_trials, trial_length, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial_dx, (rt_trial, delay_trial) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(rt, delay, trial_type)):\n\u001b[1;32m    244\u001b[0m     rt_bins \u001b[38;5;241m=\u001b[39m (rt_trial\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39m(start\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbinsize) \u001b[38;5;241m-\u001b[39m rt_bins \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedb886-4f96-43a9-bb78-8fd9d5dc6d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064c75b-8d5d-49e3-91ef-14ef4dbc1b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfd862-335a-45e8-93ea-ae29e02af8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c616d-6cf1-43aa-a982-f486110abb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opium",
   "language": "python",
   "name": "opium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
