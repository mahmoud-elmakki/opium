{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eXponential Family Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/catniplab/NeuroTask/blob/main/xfads_tutorial/xfads_tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "</br>In this part of the tutorial, we will be focusing on approximating the posterior distribution of the latent variables, given the observed data, that captures, in a low dimensional space, the population-level computations.<br>Which also known as State-space Modeling when, as in our case, it also captures the dynamic evolution of these latent factors over time, and the change of the system from a state to another.\n",
    "\n",
    "XFADS ([Dowling, Zhao, Park. 2024](https://arxiv.org/abs/2403.01371)) is a framework for inferring such kind of dynamical systems, by reaching a low-rank approximation of the poserior over latents given observations.\n",
    "\n",
    "For more info about the applications of XFADS see the public code ([GitHub repo](https://github.com/catniplab/xfads)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from NeuroTask.api_neurotask import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "# Wher to save all the resulting figuires\n",
    "!mkdir output_figs\n",
    "# Wher to save all the post processed data splits (train/val/test)\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Filter a specific Dataset\n",
    "\n",
    "Before diving into the analysis, it's essential to load the dataset and preprocess it. In this step, we'll filter out non-reward trials to focus our analysis on the relevant data.\n",
    "\n",
    "Remove trials with outcomes: Aborted (A), Incomplete (I), Failed (F)\n",
    "\n",
    "Also, return the bin size of the dataset in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of (session, animal) groups:\n",
      "\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland1_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland1_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        1       1                     1524\n",
      "   session  animal  target_pos_x  target_pos_y   count\n",
      "0        1       1           0.0           0.0  967683\n",
      "1        1       1         132.0          56.0  892769\n",
      "2        1       1         132.0          46.0  891918\n",
      "3        1       1         111.0          85.0  848778\n",
      "4        1       1           0.0          81.0  355087\n",
      "5        1       1         133.0           0.0  283099\n",
      "6        1       1           0.0          86.0  268623\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland2_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland2_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        2       1                     2203\n",
      "   session  animal  target_pos_x  target_pos_y    count\n",
      "0        2       1         123.0         -81.0  1373587\n",
      "1        2       1         133.0         -81.0  1321649\n",
      "2        2       1         132.0          56.0  1188039\n",
      "3        2       1        -118.0         -83.0   495151\n",
      "4        2       1        -130.0         -13.0   472128\n",
      "5        2       1         -77.0          82.0   458138\n",
      "6        2       1         131.0         -55.0   453295\n",
      "7        2       1         123.0          71.0   448149\n",
      "8        2       1        -118.0          -7.0   424679\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland3_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland3_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        3       1                     2537\n",
      "    session  animal  target_pos_x  target_pos_y   count\n",
      "0         3       1         132.0          56.0  663540\n",
      "1         3       1         133.0         -81.0  502578\n",
      "2         3       1        -109.0           2.0  437141\n",
      "3         3       1         111.0          85.0  425527\n",
      "4         3       1          94.0         -86.0  330094\n",
      "5         3       1        -108.0          65.0  326508\n",
      "6         3       1         132.0          46.0  325377\n",
      "7         3       1        -111.0         -82.0  325025\n",
      "8         3       1         123.0         -81.0  307555\n",
      "9         3       1           2.0          84.0  305902\n",
      "10        3       1        -123.0          25.0  283076\n",
      "11        3       1        -116.0          -5.0  221476\n",
      "12        3       1           2.0          82.0  200516\n",
      "13        3       1        -118.0         -83.0  188853\n",
      "14        3       1         -77.0          82.0  177537\n",
      "15        3       1         132.0         -65.0  174290\n",
      "16        3       1         133.0         -80.0  173977\n",
      "17        3       1        -108.0          81.0  150409\n",
      "18        3       1         118.0          72.0  141317\n",
      "19        3       1         -82.0         -86.0  139251\n",
      "20        3       1         123.0          71.0  138208\n",
      "21        3       1         -55.0         -81.0  138032\n",
      "22        3       1         129.0         -29.0  135255\n",
      "23        3       1         131.0         -55.0  134687\n",
      "24        3       1        -121.0          81.0  134312\n",
      "25        3       1        -105.0         -76.0  133823\n",
      "26        3       1         113.0         -17.0  130790\n",
      "27        3       1        -118.0          -7.0  130129\n",
      "28        3       1        -130.0         -13.0  129466\n",
      "29        3       1         116.0         -77.0  123008\n",
      "30        3       1          27.0          82.0  121652\n",
      "31        3       1          -9.0          86.0  119406\n",
      "32        3       1         131.0         -85.0  118172\n",
      "33        3       1        -116.0         -77.0  111619\n",
      "34        3       1         113.0          79.0  104512\n",
      "35        3       1        -130.0         -57.0    3226\n",
      "36        3       1         120.0          70.0    2082\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland4_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland4_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        4       1                     2249\n",
      "   session  animal  target_pos_x  target_pos_y    count\n",
      "0        4       1        -123.0          25.0  1330921\n",
      "1        4       1        -108.0          65.0  1307778\n",
      "2        4       1           2.0          84.0  1261575\n",
      "3        4       1         116.0         -77.0   509510\n",
      "4        4       1        -105.0         -76.0   505089\n",
      "5        4       1         113.0         -17.0   482058\n",
      "6        4       1         129.0         -29.0   479685\n",
      "7        4       1         131.0         -85.0   458908\n",
      "8        4       1         113.0          79.0   388128\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland5_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland5_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        1       2                     2968\n",
      "    session  animal  target_pos_x  target_pos_y    count\n",
      "0         1       2           0.0           0.0  3272022\n",
      "1         1       2         121.0           0.0  1863710\n",
      "2         1       2         114.0          91.0  1834808\n",
      "3         1       2         125.0           0.0   632814\n",
      "4         1       2           0.0         111.0   606934\n",
      "..      ...     ...           ...           ...      ...\n",
      "69        1       2          66.0          52.0     2457\n",
      "70        1       2           0.0         106.0     2422\n",
      "71        1       2          74.0           0.0     2367\n",
      "72        1       2           0.0          79.0     2312\n",
      "73        1       2          68.0         100.0     2287\n",
      "\n",
      "[74 rows x 5 columns]\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland6_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland6_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        2       2                     3029\n",
      "     session  animal  target_pos_x  target_pos_y    count\n",
      "0          2       2         114.0          91.0  1717650\n",
      "1          2       2        -115.0        -100.0  1601487\n",
      "2          2       2         121.0         -76.0  1389102\n",
      "3          2       2        -136.0         -23.0   590590\n",
      "4          2       2         -13.0         111.0   565475\n",
      "..       ...     ...           ...           ...      ...\n",
      "101        2       2          83.0          91.0     2177\n",
      "102        2       2          74.0          97.0     2167\n",
      "103        2       2         -87.0          71.0     2162\n",
      "104        2       2        -123.0          74.0     2157\n",
      "105        2       2          62.0          99.0     2102\n",
      "\n",
      "[106 rows x 5 columns]\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland7_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland7_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        3       2                     2843\n",
      "     session  animal  target_pos_x  target_pos_y   count\n",
      "0          3       2         -90.0         -82.0  481180\n",
      "1          3       2        -106.0         -80.0  473388\n",
      "2          3       2         115.0         -80.0  454460\n",
      "3          3       2          95.0         -89.0  451488\n",
      "4          3       2         141.0           6.0  446659\n",
      "..       ...     ...           ...           ...     ...\n",
      "113        3       2        -112.0          80.0    2567\n",
      "114        3       2          65.0          91.0    2567\n",
      "115        3       2          31.0          73.0    2482\n",
      "116        3       2          82.0          39.0    2447\n",
      "117        3       2        -141.0         -67.0    2432\n",
      "\n",
      "[118 rows x 5 columns]\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland8_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland8_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        4       2                     2882\n",
      "   session  animal  target_pos_x  target_pos_y    count\n",
      "0        4       2        -119.0         -38.0  4175675\n",
      "1        4       2        -107.0          78.0  4103674\n",
      "192\n",
      "Data loaded from /Users/mahmoud/finalnt/NeuroTask/NTBDataset/6_1_Churchland9_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n",
      "\n",
      "6_1_Churchland9_Maze.parquet: 1\n",
      "\n",
      "   session  animal  unique_trials_per_group\n",
      "0        5       2                     2882\n",
      "   session  animal  target_pos_x  target_pos_y    count\n",
      "0        5       2        -119.0         -38.0  4175675\n",
      "1        5       2        -107.0          78.0  4103674\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_files = ['6_1_Churchland1_Maze.parquet', '6_1_Churchland2_Maze.parquet', '6_1_Churchland3_Maze.parquet', '6_1_Churchland4_Maze.parquet', '6_1_Churchland5_Maze.parquet', '6_1_Churchland6_Maze.parquet', '6_1_Churchland7_Maze.parquet', '6_1_Churchland8_Maze.parquet', '6_1_Churchland9_Maze.parquet']\n",
    "\n",
    "print('\\number of (session, animal) groups:\\n')\n",
    "for data_file in data_files:\n",
    "    dataset, bin = load_and_filter_parquet(f'/Users/mahmoud/finalnt/NeuroTask/NTBDataset/{data_file}', ['A', 'I','F'])\n",
    "    print(f\"\\n{data_file}: {len(dataset.groupby(['session', 'animal'])['trial_id'].nunique())}\\n\")\n",
    "    dataset = dataset.dropna(axis=1)\n",
    "    print(dataset.groupby(['session', 'animal'])['trial_id'].nunique().reset_index(name='unique_trials_per_group'))\n",
    "    print(dataset.groupby(['session', 'animal'])[['target_pos_x', 'target_pos_y']].value_counts().reset_index(name='count'))\n",
    "    neuron_columns = [col for col in dataset.columns if col.startswith('Neuron')]\n",
    "    print(len(neuron_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data/NTB/6_1_Churchland4_Maze.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_end', 'EventMovement_start']\n",
      "Covariates columns: ['hand_pos_x', 'hand_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y', 'maze_num_target', 'maze_num_barriers', 'cursor_vel_x', 'cursor_vel_y']\n"
     ]
    }
   ],
   "source": [
    "# Note: the folder name that contains the data files should not contain '_'\n",
    "parquet_file_path = 'data/NTB/6_1_Churchland4_Maze.parquet'\n",
    "dataset, orig_bin_size = load_and_filter_parquet(parquet_file_path, [])\n",
    "orig_bin_size = int(orig_bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neuron1</th>\n",
       "      <th>Neuron2</th>\n",
       "      <th>Neuron3</th>\n",
       "      <th>Neuron4</th>\n",
       "      <th>Neuron5</th>\n",
       "      <th>Neuron6</th>\n",
       "      <th>Neuron7</th>\n",
       "      <th>Neuron8</th>\n",
       "      <th>Neuron9</th>\n",
       "      <th>Neuron10</th>\n",
       "      <th>...</th>\n",
       "      <th>result</th>\n",
       "      <th>cursor_vel_x</th>\n",
       "      <th>cursor_vel_y</th>\n",
       "      <th>datasetID</th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>task</th>\n",
       "      <th>EventGo_cue</th>\n",
       "      <th>EventMovement_end</th>\n",
       "      <th>EventMovement_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-2.480227</td>\n",
       "      <td>-15.847098</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-1.623605</td>\n",
       "      <td>-16.798061</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.187036</td>\n",
       "      <td>-18.901908</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>1.453091</td>\n",
       "      <td>-22.128978</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-28.244589</td>\n",
       "      <td>10.380020</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723648</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-26.805867</td>\n",
       "      <td>18.343453</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723649</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-21.344944</td>\n",
       "      <td>24.227694</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-14.491088</td>\n",
       "      <td>25.723073</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723651</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-9.113512</td>\n",
       "      <td>21.325760</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6723652 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Neuron1  Neuron2  Neuron3  Neuron4  Neuron5  Neuron6  Neuron7  \\\n",
       "0            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "6723647      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723648      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723649      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723650      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "6723651      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "         Neuron8  Neuron9  Neuron10  ...  result  cursor_vel_x  cursor_vel_y  \\\n",
       "0            0.0      0.0       1.0  ...       R      0.000000      0.000000   \n",
       "1            0.0      1.0       0.0  ...       R     -2.480227    -15.847098   \n",
       "2            0.0      0.0       0.0  ...       R     -1.623605    -16.798061   \n",
       "3            0.0      0.0       0.0  ...       R     -0.187036    -18.901908   \n",
       "4            0.0      0.0       0.0  ...       R      1.453091    -22.128978   \n",
       "...          ...      ...       ...  ...     ...           ...           ...   \n",
       "6723647      0.0      0.0       0.0  ...       R    -28.244589     10.380020   \n",
       "6723648      0.0      0.0       0.0  ...       R    -26.805867     18.343453   \n",
       "6723649      0.0      0.0       0.0  ...       R    -21.344944     24.227694   \n",
       "6723650      0.0      0.0       0.0  ...       R    -14.491088     25.723073   \n",
       "6723651      0.0      0.0       0.0  ...       R     -9.113512     21.325760   \n",
       "\n",
       "         datasetID  session  animal  task  EventGo_cue  EventMovement_end  \\\n",
       "0                6        4       1  Maze        False              False   \n",
       "1                6        4       1  Maze        False              False   \n",
       "2                6        4       1  Maze        False              False   \n",
       "3                6        4       1  Maze        False              False   \n",
       "4                6        4       1  Maze        False              False   \n",
       "...            ...      ...     ...   ...          ...                ...   \n",
       "6723647          6        4       1  Maze        False              False   \n",
       "6723648          6        4       1  Maze        False              False   \n",
       "6723649          6        4       1  Maze        False              False   \n",
       "6723650          6        4       1  Maze        False              False   \n",
       "6723651          6        4       1  Maze        False              False   \n",
       "\n",
       "         EventMovement_start  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "...                      ...  \n",
       "6723647                False  \n",
       "6723648                False  \n",
       "6723649                False  \n",
       "6723650                False  \n",
       "6723651                False  \n",
       "\n",
       "[6723652 rows x 211 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Maze'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['task'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>target_pos_x</th>\n",
       "      <th>target_pos_y</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1330921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1307778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1261575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>509510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>505089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>482058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>479685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>131.0</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>458908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>388128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  animal  target_pos_x  target_pos_y    count\n",
       "0        4       1        -123.0          25.0  1330921\n",
       "1        4       1        -108.0          65.0  1307778\n",
       "2        4       1           2.0          84.0  1261575\n",
       "3        4       1         116.0         -77.0   509510\n",
       "4        4       1        -105.0         -76.0   505089\n",
       "5        4       1         113.0         -17.0   482058\n",
       "6        4       1         129.0         -29.0   479685\n",
       "7        4       1         131.0         -85.0   458908\n",
       "8        4       1         113.0          79.0   388128"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique target locations (can be used later for classification or behaviour decoding).\n",
    "dataset.groupby(['session', 'animal'])[['target_pos_x', 'target_pos_y']].value_counts().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>unique_trials_per_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  animal  unique_trials_per_group\n",
       "0        4       1                        1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['session', 'animal'])['result'].nunique().reset_index(name='unique_trials_per_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial_id\n",
      "hand_pos_x\n",
      "hand_pos_y\n",
      "cursor_pos_x\n",
      "cursor_pos_y\n",
      "target_pos_x\n",
      "target_pos_y\n",
      "maze_num_target\n",
      "maze_num_barriers\n",
      "result\n",
      "cursor_vel_x\n",
      "cursor_vel_y\n",
      "datasetID\n",
      "session\n",
      "animal\n",
      "task\n",
      "EventGo_cue\n",
      "EventMovement_end\n",
      "EventMovement_start\n"
     ]
    }
   ],
   "source": [
    "for column in dataset.columns:\n",
    "    if \"Neuron\" not in column:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Maze'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['task'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['datasetID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>unique_trials_per_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  animal  unique_trials_per_group\n",
       "0        4       1                     2249"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(['session', 'animal'])['trial_id'].nunique().reset_index(name='unique_trials_per_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>target_pos_x</th>\n",
       "      <th>target_pos_y</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1330921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1307778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1261575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>509510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>505089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>482058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>479685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>131.0</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>458908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>388128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  animal  target_pos_x  target_pos_y    count\n",
       "0        4       1        -123.0          25.0  1330921\n",
       "1        4       1        -108.0          65.0  1307778\n",
       "2        4       1           2.0          84.0  1261575\n",
       "3        4       1         116.0         -77.0   509510\n",
       "4        4       1        -105.0         -76.0   505089\n",
       "5        4       1         113.0         -17.0   482058\n",
       "6        4       1         129.0         -29.0   479685\n",
       "7        4       1         131.0         -85.0   458908\n",
       "8        4       1         113.0          79.0   388128"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique target locations (can be used later for classification or behaviour decoding).\n",
    "dataset.groupby(['session', 'animal'])[['target_pos_x', 'target_pos_y']].value_counts().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neuron1</th>\n",
       "      <th>Neuron2</th>\n",
       "      <th>Neuron3</th>\n",
       "      <th>Neuron4</th>\n",
       "      <th>Neuron5</th>\n",
       "      <th>Neuron6</th>\n",
       "      <th>Neuron7</th>\n",
       "      <th>Neuron8</th>\n",
       "      <th>Neuron9</th>\n",
       "      <th>Neuron10</th>\n",
       "      <th>...</th>\n",
       "      <th>result</th>\n",
       "      <th>cursor_vel_x</th>\n",
       "      <th>cursor_vel_y</th>\n",
       "      <th>datasetID</th>\n",
       "      <th>session</th>\n",
       "      <th>animal</th>\n",
       "      <th>task</th>\n",
       "      <th>EventGo_cue</th>\n",
       "      <th>EventMovement_end</th>\n",
       "      <th>EventMovement_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-2.480227</td>\n",
       "      <td>-15.847098</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-1.623605</td>\n",
       "      <td>-16.798061</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.187036</td>\n",
       "      <td>-18.901908</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>1.453091</td>\n",
       "      <td>-22.128978</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-28.244589</td>\n",
       "      <td>10.380020</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723648</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-26.805867</td>\n",
       "      <td>18.343453</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723649</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-21.344944</td>\n",
       "      <td>24.227694</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-14.491088</td>\n",
       "      <td>25.723073</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723651</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>-9.113512</td>\n",
       "      <td>21.325760</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Maze</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6723652 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Neuron1  Neuron2  Neuron3  Neuron4  Neuron5  Neuron6  Neuron7  \\\n",
       "0            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4            0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "6723647      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723648      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723649      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "6723650      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "6723651      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "         Neuron8  Neuron9  Neuron10  ...  result  cursor_vel_x  cursor_vel_y  \\\n",
       "0            0.0      0.0       1.0  ...       R      0.000000      0.000000   \n",
       "1            0.0      1.0       0.0  ...       R     -2.480227    -15.847098   \n",
       "2            0.0      0.0       0.0  ...       R     -1.623605    -16.798061   \n",
       "3            0.0      0.0       0.0  ...       R     -0.187036    -18.901908   \n",
       "4            0.0      0.0       0.0  ...       R      1.453091    -22.128978   \n",
       "...          ...      ...       ...  ...     ...           ...           ...   \n",
       "6723647      0.0      0.0       0.0  ...       R    -28.244589     10.380020   \n",
       "6723648      0.0      0.0       0.0  ...       R    -26.805867     18.343453   \n",
       "6723649      0.0      0.0       0.0  ...       R    -21.344944     24.227694   \n",
       "6723650      0.0      0.0       0.0  ...       R    -14.491088     25.723073   \n",
       "6723651      0.0      0.0       0.0  ...       R     -9.113512     21.325760   \n",
       "\n",
       "         datasetID  session  animal  task  EventGo_cue  EventMovement_end  \\\n",
       "0                6        4       1  Maze        False              False   \n",
       "1                6        4       1  Maze        False              False   \n",
       "2                6        4       1  Maze        False              False   \n",
       "3                6        4       1  Maze        False              False   \n",
       "4                6        4       1  Maze        False              False   \n",
       "...            ...      ...     ...   ...          ...                ...   \n",
       "6723647          6        4       1  Maze        False              False   \n",
       "6723648          6        4       1  Maze        False              False   \n",
       "6723649          6        4       1  Maze        False              False   \n",
       "6723650          6        4       1  Maze        False              False   \n",
       "6723651          6        4       1  Maze        False              False   \n",
       "\n",
       "         EventMovement_start  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "...                      ...  \n",
       "6723647                False  \n",
       "6723648                False  \n",
       "6723649                False  \n",
       "6723650                False  \n",
       "6723651                False  \n",
       "\n",
       "[6723652 rows x 211 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop neurons with no spikes\n",
    "dataset = dataset.dropna(axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Probably not, unless the data is already preprocessed, but let's check if all trials are the same length.\n",
    "for trial_idx in range(len([trial[1] for trial in dataset.groupby('trial_id')])-1):\n",
    "    if len(dataset.loc[dataset['trial_id'] == trial_idx]) != len(dataset.loc[dataset['trial_id'] == trial_idx+1]):\n",
    "        is_same = False\n",
    "        break\n",
    "print(is_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rebin Data for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rebin` function is designed to rebin a dataset by aggregating data points into larger bins based on a specified bin size. Here's a brief overview of how to use it:\n",
    "\n",
    "The `rebin` function takes the following parameters:\n",
    "- `dataset1`: The DataFrame containing the data to be rebinned.\n",
    "- `prev_bin_size`: The original bin size of the data.\n",
    "- `new_bin_size`: The desired bin size to aggregate data points into. This is the new bin size you want the data to be rebinned to.\n",
    "- `reset` (optional): A boolean indicating whether to reset the index of the resulting DataFrame.\n",
    "\n",
    "When called, the function aggregates data points within each bin based on the specified aggregation functions. The aggregation functions are determined based on the column names of the input DataFrame. For spiking data, the function aggregates by summing the values. For columns related to behavior, the function applies a custom decimation function to downsample the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rebin the dataset with a new bin size (in ms).\n",
    "dataset = rebin(dataset, prev_bin_size=orig_bin_size, new_bin_size=5)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_len_df = pd.DataFrame(columns=['session', 'animal', 'n_trials', 'shoretest', 'longest', 'mean', 'std'])\n",
    "\n",
    "for group_id, group in dataset.groupby(['session', 'animal']):\n",
    "    avg_len = 0\n",
    "    len_std = 1\n",
    "    min_len = 1e6\n",
    "    max_len = 0\n",
    "    n_trials = len(group.groupby(['trial_id']))\n",
    "    \n",
    "    for _, trial in group.groupby(['trial_id']):\n",
    "        trial_len = len(trial)\n",
    "        avg_len = avg_len + (trial_len / n_trials)\n",
    "        len_std = (trial_len - avg_len) ** 2  / n_trials\n",
    "        \n",
    "        if len(trial) > max_len:\n",
    "            max_len = trial_len\n",
    "        elif len(trial) < min_len:\n",
    "            min_len = trial_len\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    trials_len_df.loc[len(trials_len_df)] = [int(group_id[0]), int(group_id[1]), n_trials, min_len, max_len, avg_len, len_std]\n",
    "    \n",
    "trials_len_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Align to specific event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the `align_event` function, it's essential to understand its purpose and how it operates. This function is designed to align events within a DataFrame based on a specified start event marker. Here's a brief overview of how to use it:\n",
    "\n",
    "The `align_event` function takes the following parameters:\n",
    "- `df`: The DataFrame containing the data.\n",
    "- `bin_size`: the bin size of the data in ms.\n",
    "- `start_event`: The event marker indicating the start of a trial or session.\n",
    "- `offset_min` (optional): The minimum offset (in ms) to consider before the start_event.\n",
    "- `offset_max` (optional): The maximum offset (in ms) to consider after the start_event.\n",
    "\n",
    "\n",
    "Please note that for Dataset 1, it's not possible to align events since it doesn't contain event information.\\\n",
    "Also note, after the alignment some trials (those are on the two ends of each (session, animal) group) will be trimmed, so we would want to get rid of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_cols = [col for col in dataset.columns if col.startswith('Event')]\n",
    "event_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the time bin of each event, in each trial.\n",
    "\"\"\"\n",
    "# List of DataFrames of session-animal combinations\n",
    "event_bins = []\n",
    "\n",
    "for group_id, group in dataset.groupby(['session', 'animal']):\n",
    "    event_bins_df = pd.DataFrame(columns=['trial_id']+event_cols)\n",
    "    \n",
    "    for trial_id, trial in group.groupby(['trial_id']):\n",
    "        trial = trial.reset_index()\n",
    "        event_bins_df.loc[len(event_bins_df)] = np.array(list([trial_id][0]) + list([trial[trial[event] == True].index[0] for event in event_cols]))\n",
    "    event_bins.append(event_bins_df)\n",
    "    \n",
    "event_bins[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When aligning, i.e. trimming trials, consider the disrtibutions of the event bins so that include as many event bins\n",
    "in as many trials as possible, for example, you might consider setting up the `offset_min` and `offset_max` so that\n",
    "the mean bin number, of the event you are aligning around, is in the middle.\n",
    "\"\"\"\n",
    "for event in event_cols:\n",
    "    plt.hist(event_bins[0][event], bins='auto', density=False, alpha=0.7, edgecolor='gray', label=f'{event} (mean: {int(np.round(event_bins[0][event].mean()))})')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"bins of events occurance\")\n",
    "plt.xlabel(\"event bin\")\n",
    "plt.ylabel(\"num of trials\")\n",
    "\n",
    "plt.savefig(\"output_figs/event_bins_dist.png\")\n",
    "\n",
    "print(f\"min bin for event occurance\\n\\n{event_bins[0].min()[event_cols]}\\n\")\n",
    "print(f\"max bin for event occurance\\n\\n{event_bins[0].max()[event_cols]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's set the offsets before and after a specific event. (((offset_min + offset_max) / bin_size ) + 1) -- 1 = current bin\n",
    "event_align = 'EventMovement_start' # The event to align the trials around (the column name)\n",
    "bin_size = 5 #ms (If rebinned, make sure this is the new bin size)\n",
    "offset_min = 1115 #ms\n",
    "offset_max = 980 #ms\n",
    "trial_length = offset_min + offset_max + 1 * bin_size # Total trial length, ms, (including the bin we aligning around)\n",
    "n_bins = trial_length // bin_size\n",
    "\n",
    "dataset_aligned = align_event(dataset, event_align, bin_size=bin_size, offset_min=-offset_min, offset_max=offset_max)\n",
    "print(f'length of aligned trials = {n_bins} bins x {bin_size} ms = {trial_length} ms\\n')\n",
    "dataset_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"The trials on the sides of each session-animal group now might be shoerter due to the trimming for the alignment,\n",
    "so from the event bins distribution make sure to include as many trials as possible in the time window you choose.\n",
    "\"\"\"\n",
    "trial_lens = []\n",
    "for t, trial in dataset_aligned.groupby(['session', 'animal', 'trial_id']):\n",
    "    trial_lens.append(len(trial))\n",
    "    \n",
    "len_counts = {}\n",
    "for l in trial_lens:\n",
    "    if f'{l} bins' in len_counts:\n",
    "        len_counts[f'{l} bins'] += 1\n",
    "    else:\n",
    "        len_counts[f'{l} bins'] = 1\n",
    "        \n",
    "print(len_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Let us see how many trials has within it each event time bin.\n",
    "\"\"\"\n",
    "_0_1 = 0\n",
    "_1_2 = 0\n",
    "_0_2 = 0\n",
    "\n",
    "for _, q in dataset_aligned.groupby(['session', 'animal', 'trial_id']):\n",
    "\n",
    "    if(q[event_cols[0]].any() and q[event_cols[1]].any()):\n",
    "        _0_1 += 1\n",
    "    if(q[event_cols[1]].any() and q[event_cols[2]].any()):\n",
    "        _1_2 += 1\n",
    "    if(q[event_cols[0]].any() and q[event_cols[2]].any()):\n",
    "        _0_2 += 1\n",
    "        \n",
    "print(event_cols[0], \"&\", event_cols[1], ':',  _0_1,  'trials')\n",
    "print(event_cols[1], \"&\", event_cols[2], ':',  _1_2,  'trials')\n",
    "print(event_cols[0], \"&\", event_cols[2], ':',  _0_2,  'trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the time bin of each event, in each trial.\n",
    "\"\"\"\n",
    "# The final result is a list of DataFrames of session-animal combinations\n",
    "aligned_event_bins = []\n",
    "aligned_event_cols = ['EventGo_cue', 'EventMovement_start', 'EventMovement_end']\n",
    "\n",
    "for group_id, group in dataset_aligned.groupby(['session', 'animal']):\n",
    "    event_bins_df = pd.DataFrame(columns = ['trial_id'] + aligned_event_cols)\n",
    "    \n",
    "    for trial_id, trial in group.groupby(['trial_id']):\n",
    "        trial = trial.reset_index()\n",
    "        events = []\n",
    "        for event in aligned_event_cols:\n",
    "            try:\n",
    "                events.append(trial[trial[event] == True].index[0])\n",
    "            except (IndexError, KeyError):\n",
    "                for gi, g in dataset.groupby(['session', 'animal']):\n",
    "                    if gi == group_id:\n",
    "                        for ti, t in g.groupby('trial_id'):\n",
    "                            if ti == trial_id[0]:\n",
    "                                events.append(t[(t['trial_id'] == ti) & (t[event] == True)].index[0])\n",
    "                                    \n",
    "        event_bins_df.loc[len(event_bins_df)] = np.array(list([trial_id][0]) + events)\n",
    "    aligned_event_bins.append(event_bins_df)\n",
    "    \n",
    "aligned_event_bins[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split and model's inputs processing, with spike history\n",
    "\n",
    "This function is designed to process a dataset, splitting it into training, validation, and testing sets, and optionally performing z-score normalization on the input data. Here's a brief overview of how to use it:\n",
    "\n",
    "The `process_data` function takes the following parameters:\n",
    "\n",
    "- `df` (pd.DataFrame): The DataFrame containing the data.\n",
    "- `bins_before` (int): Number of bins before the output used for decoding.\n",
    "- `training_range` (list): The range [start, end] for the training set.\n",
    "- `valid_range` (list): The range [start, end] for the validation set.\n",
    "- `testing_range` (list): The range [start, end] for the testing set.\n",
    "- `behavior_columns` (list): List of columns containing behavioral data.\n",
    "- `zscore` (bool): Whether to apply z-score normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[col for col in dataset.columns if any(_ in col for _ in ['vel', 'pos', 'force', 'acc', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_cols = ['hand_pos_x', 'hand_pos_y', 'cursor_vel_x', 'cursor_vel_y', 'cursor_pos_x', 'cursor_pos_y', 'target_pos_x', 'target_pos_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The result of the following line is a GroupBy object that has DataFrames as elements. Each DataFrame has all the time bins in that group as rows.\n",
    "# So we need to group each group again by 'trial_id'\n",
    "groups = [group[1] for group in dataset_aligned.groupby(['session', 'animal'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Each data split is a list np arrays, for each animal per session. And each array is a matrix\n",
    "of size \"number of total time bins\" x \"number of surrounding time bins used for prediction\" x \"number of neurons\"\n",
    "For every time bin, there are the firing rates of all neurons from the specified number of time bins before (and after).\n",
    "\"\"\"\n",
    "X_train_list, y_train_list, X_val_list, y_val_list, X_test_list, y_test_list = process_data(dataset_aligned, bins_before=6,\n",
    "                                                 training_range=[0, 0.7], valid_range=[0.7, 0.8], testing_range=[0.8, 1], \n",
    "                                                 behavior_columns=[label_cols[2], label_cols[3]], zscore = False)\n",
    "\n",
    "print(X_train_list[0].shape)\n",
    "print(y_train_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"There are\", sum(X.shape[2] for X in X_train_list), \"unique neurons in the dataset in\", len(X_train_list), \"different sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are examining the unique number of trials per session for each animal. We use the pandas `groupby` function to group the data by the 'animal' and 'session' columns, and then calculate the number of unique trial IDs within each group using the `nunique` function. Finally, we reset the index of the resulting DataFrame and rename the column to 'unique_trials_per_session' for clarity. This analysis allows us to understand the distribution of trial counts across different sessions and animals in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_aligned.groupby(['session', 'animal'])['trial_id'].nunique().reset_index(name='unique_trials_per_session')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to visualize one trial's neural activity alongside hand movement velocity. This code generates a raster plot and plots velocity data for a specific trial in the dataset. First, it filters the dataset to select the data for a particular trial based on the trial ID, session, and animal. It then identifies the columns corresponding to neurons and event timings. Using this information, it creates a raster plot to visualize the firing activity of neurons over time, with each row representing a different neuron. Additionally, it plots vertical lines to indicate the occurrence of specific events such as 'Go_cue', 'Target_Onset', and 'Bump_time'. The x-axis ticks are labeled with event indications to show the timing of these events. Furthermore, velocity data for the hand movement in the x and y directions is plotted on the secondary y-axis. This combined visualization provides insights into the neural activity and hand movement dynamics during the trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_id = 66\n",
    "trial_data = dataset_aligned[\n",
    "    (dataset_aligned['trial_id'] == trial_id) & (dataset_aligned['session'] == 5) & (dataset_aligned['animal'] == 2)\n",
    "]\n",
    "\n",
    "# Select data for neurons\n",
    "neuron_columns = [col for col in trial_data.columns if col.startswith('Neuron')]\n",
    "neurons = neuron_columns[:]\n",
    "\n",
    "# Identify event columns\n",
    "event_columns = [col for col in dataset_aligned.columns if col.startswith('Event')]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot events per time for the selected neurons\n",
    "for neuron in neurons:\n",
    "    events = trial_data.index[trial_data[neuron] == 1].tolist()\n",
    "    ax.eventplot(events, lineoffsets=neurons.index(neuron), linelengths=0.5, color='black')\n",
    "\n",
    "# Plot event indications as vertical lines for each event type\n",
    "colors = ['red', 'blue', 'green']\n",
    "event_labels = []\n",
    "event_positions = []\n",
    "\n",
    "for idx, event_col in enumerate(event_columns):\n",
    "    event_indices = trial_data.index[trial_data[event_col] == 1].tolist()\n",
    "    for event_index in event_indices:\n",
    "        ax.axvline(x=event_index, linestyle='-', label=event_col)\n",
    "        event_positions.append(event_index)\n",
    "        event_labels.append(event_col)\n",
    "\n",
    "# Combine event labels for positions with multiple events\n",
    "event_dict = {}\n",
    "for pos, label in zip(event_positions, event_labels):\n",
    "    if pos in event_dict:\n",
    "        event_dict[pos].add(label)\n",
    "    else:\n",
    "        event_dict[pos] = {label}\n",
    "\n",
    "# Sort events to ensure the ticks are in order\n",
    "sorted_event_positions = sorted(event_dict.keys())\n",
    "sorted_event_labels = [', '.join(event_dict[pos]) for pos in sorted_event_positions]\n",
    "\n",
    "# Set x-axis ticks with event indications\n",
    "ax.set_xticks(sorted_event_positions)\n",
    "ax.set_xticklabels(sorted_event_labels, rotation=45, fontsize=10)\n",
    "\n",
    "# Plot velocity data for y-axis\n",
    "ax2 = ax.twinx()\n",
    "#for label_col in label_cols:\n",
    "#    ax2.plot(trial_data[label_col], label=label_col)\n",
    "ax2.plot(trial_data[label_cols[0]], color='black', label=label_cols[0])\n",
    "ax2.plot(trial_data[label_cols[1]], color='blue', label=label_cols[1])\n",
    "\n",
    "# Set y-axis label for velocity\n",
    "ax2.set_ylabel('label value', fontsize=12)\n",
    "\n",
    "ax.set_ylabel('neuron', fontsize=12)\n",
    "ax.set_title(f'Raster Plot for Trial {trial_id}\\n', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_id = 66\n",
    "trial_data = dataset[(dataset['trial_id'] == trial_id) & (dataset['session'] == 5) & (dataset['animal'] == 2)]\n",
    "\n",
    "# Select data for neurons\n",
    "neuron_columns = [col for col in trial_data.columns if col.startswith('Neuron')]\n",
    "neurons = neuron_columns[:]\n",
    "\n",
    "# Identify event columns\n",
    "event_columns = [col for col in dataset.columns if col.startswith('Event')]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot events per time for the selected neurons\n",
    "for neuron in neurons:\n",
    "    events = trial_data.index[trial_data[neuron] == 1].tolist()\n",
    "    ax.eventplot(events, lineoffsets=neurons.index(neuron), alpha=1.0, linelengths=0.5, color='black')\n",
    "\n",
    "# Plot event indications as vertical lines for each event type\n",
    "colors = ['red', 'blue', 'green']\n",
    "event_labels = []\n",
    "event_positions = []\n",
    "\n",
    "for idx, event_col in enumerate(event_columns):\n",
    "    event_indices = trial_data.index[trial_data[event_col] == 1].tolist()\n",
    "    for event_index in event_indices:\n",
    "        ax.axvline(x=event_index, linestyle='-', label=event_col)\n",
    "        event_positions.append(event_index)\n",
    "        event_labels.append(event_col)\n",
    "\n",
    "# Combine event labels for positions with multiple events\n",
    "event_dict = {}\n",
    "for pos, label in zip(event_positions, event_labels):\n",
    "    if pos in event_dict:\n",
    "        event_dict[pos].add(label)\n",
    "    else:\n",
    "        event_dict[pos] = {label}\n",
    "\n",
    "# Sort events to ensure the ticks are in order\n",
    "sorted_event_positions = sorted(event_dict.keys())\n",
    "sorted_event_labels = [', '.join(event_dict[pos]) for pos in sorted_event_positions]\n",
    "\n",
    "# Set x-axis ticks with event indications\n",
    "ax.set_xticks(sorted_event_positions)\n",
    "ax.set_xticklabels(sorted_event_labels, rotation=45, fontsize=10)\n",
    "\n",
    "# Plot velocity data for y-axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(trial_data[label_cols[2]], color='black', label=label_cols[2])\n",
    "ax2.plot(trial_data[label_cols[3]], color='blue', label=label_cols[3])\n",
    "\n",
    "# Set y-axis label for velocity\n",
    "ax2.set_ylabel('label value', fontsize=12)\n",
    "\n",
    "ax.set_ylabel('neuron', fontsize=12)\n",
    "ax.set_title(f'Raster Plot for Trial {trial_id}\\n', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fitting baselines\n",
    "To check more baselines, you can go to the Notebook of the baselines tutorial. Here we are fitting a **Wiener Filter**.\n",
    "\n",
    "After, we will apply the state-space modelling and latent dynamics inferrence framework: XFADS (eXponential FAmily Dynamical Systems: **XFADS (Large-scale nonlinear Gaussian state-space modeling)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.metrics import get_R2\n",
    "from baselines.decoders import *\n",
    "\n",
    "for i in range(len(X_train_list)):\n",
    "    X_flat_train = X_train_list[i].reshape(X_train_list[i].shape[0], (X_train_list[i].shape[1]*X_train_list[i].shape[2]))\n",
    "    X_flat_test = X_test_list[i].reshape(X_test_list[i].shape[0],(X_test_list[i].shape[1]*X_test_list[i].shape[2]))\n",
    "\n",
    "    #Declare model\n",
    "    model_wf = WienerFilterDecoder()\n",
    "    #Fit model|\n",
    "    model_wf.fit(X_flat_train, y_train_list[i])\n",
    "    #Get predictions\n",
    "    y_valid_predicted_wf=model_wf.predict(X_flat_test)\n",
    "    #Get metric of fit\n",
    "    R2s_wf = get_R2(y_test_list[i], y_valid_predicted_wf)\n",
    "    \n",
    "    print('session', i+1, 'R2s:', R2s_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neurons = [col for col in dataset.columns if col.startswith('Neuron')]\n",
    "bins_before = 5\n",
    "bins_after = 2\n",
    "# List of lists of n_bins x n_neurons np arrays of the spikes of each trial.\n",
    "# session_trials[1][12] => An np array of the 12th trial spikes from the 2nd session.\n",
    "sa_groups = []\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Iterate over each unique animal in the dataset\n",
    "for a in dataset['animal'].unique():\n",
    "    # Select data for the current animal\n",
    "    d = dataset[dataset['animal'] == a]\n",
    "\n",
    "    # Iterate over each session for the current animal\n",
    "    for session in d['session'].unique():\n",
    "        # Select data for the current session\n",
    "        df_session = dataset[(dataset['animal'] == a) & (dataset['session'] == session)].dropna(axis=1)\n",
    "        session_spikes = [trial[neurons].to_numpy() for _, trial in df_session.groupby('trial_id')]\n",
    "        sa_groups.append(session_spikes)\n",
    "        \n",
    "        # Filter out zero Neuron columns\n",
    "        df_spikes = df_session[neurons]\n",
    "        spikes = df_spikes.loc[:, (df_session != 0).any(axis=0)].to_numpy()\n",
    "        \n",
    "        # Note that here we are using the labels in the indices 2 and 3 fromm the label columns.\n",
    "        X.append(get_spikes_with_history(spikes, bins_before, bins_after, bins_current=1))\n",
    "        Y.append(np.array(dataset[(dataset['session'] == session) & (dataset['animal'] == a)][label_cols[2:4]]))\n",
    "\n",
    "print(X[0].shape)\n",
    "print(Y[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming trials and labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keys: index of (session, animal) group\n",
    "# values: [trials x time bins x neurons, or labels x and y]\n",
    "trials_data = {}\n",
    "labels_data = {}\n",
    "\n",
    "for group_id, group in enumerate(groups):\n",
    "    trials = []\n",
    "    labels_data[group_id] = {}\n",
    "    trials_labels = []\n",
    "    \n",
    "    for trial_id, trial in group.groupby('trial_id'):\n",
    "        # Just add the trials with full length, n_bins\n",
    "        if len(trial) == n_bins:\n",
    "            trials.append(trial.filter(like='Neuron').values)\n",
    "            trials_labels.append(trial[label_cols].values)\n",
    "        # If trimmed because of the alignment, ignore\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # Groups with just trials that survived the trial length check\n",
    "    trials_data[group_id] = np.stack(trials)\n",
    "    trials_labels = np.stack(trials_labels)\n",
    "    labels_data[group_id] = {label: trials_labels[:, :, i] for i, label in enumerate(label_cols)}\n",
    "\n",
    "# Double check shapes\n",
    "print([trials_data[g].shape for g in range(len(groups))])\n",
    "print(labels_data[0].keys())\n",
    "print([labels_data[g][l].shape for g in range(len(groups)) for l in label_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Chosing one session-animal group.\n",
    "y = trials_data[0]\n",
    "label = labels_data[0]\n",
    "\n",
    "!mkdir data\n",
    "save_root_path = 'data/'\n",
    "\n",
    "train_data, valid_data, test_data = {}, {}, {}\n",
    "n_trials, seq_len, n_neurons = y.shape\n",
    "n_valid_trials = 640\n",
    "\n",
    "# obs: observations\n",
    "train_data['y_obs'] = torch.Tensor(y[:-n_valid_trials])\n",
    "valid_data['y_obs'] = torch.Tensor(y[-n_valid_trials:-n_valid_trials // 2])\n",
    "test_data['y_obs'] = torch.Tensor(y[-n_valid_trials // 2:])\n",
    "\n",
    "# 'n_bins_enc': Number of time bins used later by in modeling for enconding (default full trial).\n",
    "# 'n_bins_obs': originaly observed trial length (after alignment)\n",
    "# Same for 'n_neurons_obs' and 'n_neurons_enc'.\n",
    "train_data['n_bins_obs'] = valid_data['n_bins_obs'] = test_data['n_bins_obs'] = seq_len\n",
    "train_data['n_bins_enc'] = valid_data['n_bins_enc'] = test_data['n_bins_enc'] = seq_len\n",
    "train_data['n_neurons_obs'] = valid_data['n_neurons_obs'] = test_data['n_neurons_obs'] = n_neurons\n",
    "train_data['n_neurons_enc'] = valid_data['n_neurons_enc'] = test_data['n_neurons_enc'] = n_neurons\n",
    "\n",
    "# Save a 1D array for event bins for each data split, for each trial, for each event.\n",
    "# Note: the o here in event_bins[0] is the session-animal group.\n",
    "for event in event_cols:\n",
    "    train_data[event] = torch.Tensor(np.array(aligned_event_bins[0][event][:-n_valid_trials]))\n",
    "    valid_data[event] = torch.Tensor(np.array(aligned_event_bins[0][event][-n_valid_trials:-n_valid_trials // 2]))\n",
    "    test_data[event] = torch.Tensor(np.array(aligned_event_bins[0][event][-n_valid_trials // 2:]))\n",
    "\n",
    "for label in label_cols:\n",
    "    train_data[label] = torch.Tensor(np.array(labels_data[0][label][:-n_valid_trials]))\n",
    "    valid_data[label] = torch.Tensor(np.array(labels_data[0][label][-n_valid_trials:-n_valid_trials // 2]))\n",
    "    test_data[label] = torch.Tensor(np.array(labels_data[0][label][-n_valid_trials // 2:]))\n",
    "\n",
    "torch.save(train_data, save_root_path + f'data_train_{bin_size}ms.pt')\n",
    "torch.save(valid_data, save_root_path + f'data_valid_{bin_size}ms.pt')\n",
    "torch.save(test_data, save_root_path + f'data_test_{bin_size}ms.pt')\n",
    "\n",
    "print('Data splits (train/valid/test) saved into the \"data\" folder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting XFADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone XFADS repo and install its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_running_in_colab():\n",
    "    %cd content\n",
    "    !git clone https://github.com/catniplab/NeuroTask.git\n",
    "    !pip install tensorflow bayesian-optimization\n",
    "    %cd content/NeuroTask\n",
    "    !git clone https://github.com/catniplab/xfads.git\n",
    "    %cd content/NeuroTask/xfads\n",
    "    !pip install pytorch-lightning scikit-learn seaborn hydra-core matplotlib einops nlb-tools dandi\n",
    "    !pip install -e .\n",
    "    %cd content/NeuroTask\n",
    "'''    \n",
    "else:\n",
    "    !git clone https://github.com/catniplab/xfads.git\n",
    "    !pip install pytorch-lightning scikit-learn seaborn hydra-core matplotlib einops nlb-tools dandi\n",
    "    !pip install -e .\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import pytorch_lightning as lightning\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, GradientAccumulationScheduler\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.plugins.precision import deepspeed\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if is_running_in_colab():\n",
    "  %cd /content/NeuroTask \n",
    "\n",
    "import xfads.utils as utils\n",
    "import xfads.prob_utils as prob_utils\n",
    "\n",
    "from xfads import plot_utils\n",
    "\n",
    "from xfads.ssm_modules.likelihoods import PoissonLikelihood\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianDynamics\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianInitialCondition\n",
    "from xfads.ssm_modules.encoders import LocalEncoderLRMvn, BackwardEncoderLRMvn\n",
    "from xfads.smoothers.lightning_trainers import LightningNonlinearSSM, LightningMonkeyReaching\n",
    "from xfads.smoothers.nonlinear_smoother_causal import NonlinearFilter, LowRankNonlinearStateSpaceModel\n",
    "\n",
    "from xfads.ssm_modules.prebuilt_models import create_xfads_poisson_log_link\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"config\"\"\"\n",
    "\n",
    "cfg = {\n",
    "    # --- graphical model --- #\n",
    "    'n_latents': 40,\n",
    "    'n_latents_read': 35,\n",
    "\n",
    "    'rank_local': 15,\n",
    "    'rank_backward': 5,\n",
    "\n",
    "    'n_hidden_dynamics': 128,\n",
    "\n",
    "    # --- inference network --- #\n",
    "    'n_samples': 25,\n",
    "    'n_hidden_local': 256,\n",
    "    'n_hidden_backward': 128,\n",
    "\n",
    "    # --- hyperparameters --- #\n",
    "    'use_cd': False,\n",
    "    'p_mask_a': 0.0,\n",
    "    'p_mask_b': 0.0,\n",
    "    'p_mask_apb': 0.0,\n",
    "    'p_mask_y_in': 0.0,\n",
    "    'p_local_dropout': 0.4,\n",
    "    'p_backward_dropout': 0.0,\n",
    "\n",
    "    # --- training --- #\n",
    "    'device': 'cpu',\n",
    "    'data_device': 'cpu',\n",
    " \n",
    "    'lr': 1e-3,\n",
    "    'n_epochs': 1000,\n",
    "    'batch_sz': 32,\n",
    "    'minibatch_sz': 8,\n",
    "    'use_minibatching': False,\n",
    "\n",
    "    # --- misc --- #\n",
    "    'bin_sz': 5e-3,\n",
    "    'bin_sz_ms': 5,\n",
    "\n",
    "    'seed': 1236,\n",
    "    'default_dtype': torch.float32,\n",
    "    \n",
    "    'shuffle_train': True,\n",
    "    'shuffle_valid': False,\n",
    "    'shuffle_test': False,\n",
    "    \n",
    "    # --- ray --- #\n",
    "    'n_ray_samples': 10,\n",
    "}\n",
    "\n",
    "class Cfg(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self:\n",
    "            return self[attr]\n",
    "        else:\n",
    "            raise AttributeError(f\"'DictAsAttributes' object has no attribute '{attr}'\")\n",
    "\n",
    "cfg = Cfg(cfg)\n",
    "\n",
    "lightning.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'data/data_{split}_{bin_size_ms}ms.pt'\n",
    "train_data = torch.load(data_path.format(split='train', bin_size_ms=cfg.bin_sz_ms))\n",
    "val_data = torch.load(data_path.format(split='valid', bin_size_ms=cfg.bin_sz_ms))\n",
    "test_data = torch.load(data_path.format(split='test', bin_size_ms=cfg.bin_sz_ms))\n",
    "\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obs: observations\n",
    "y_train_obs = train_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_valid_obs = val_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_test_obs = test_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "\n",
    "# l: label\n",
    "labels = ['cursor_vel_x', 'cursor_vel_y']\n",
    "l_train = torch.tensor(np.array([train_data[l] for l in labels])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)\n",
    "l_valid = torch.tensor(np.array([val_data[l] for l in labels])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)\n",
    "l_test = torch.tensor(np.array([test_data[l] for l in labels])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)\n",
    "\n",
    "# Time bins of the occurance of events. One of these, usually the one that was used to align the trials around, has a fixed value.\n",
    "# IMPORTANT: These event names in this list should be the same as the keys in the train_data Dictionary.\n",
    "events = ['EventGo_cue', 'EventMovement_start', 'EventMovement_end']\n",
    "# Event names that appear on pltos\n",
    "events_str = ['go cue', 'move\\nstarts', 'move\\nends']\n",
    "# How to mark the time point where the perdection of the latents starts (just in the regime of unrolling the latents from a small number of time bins)\n",
    "pred_str = ['pred\\nstarts']\n",
    "# b: behaviour\n",
    "b_train = torch.tensor(np.array([train_data[b] for b in events])).permute(1, 0).type(torch.float32).to(cfg.data_device)\n",
    "b_valid = torch.tensor(np.array([val_data[b] for b in events])).permute(1, 0).type(torch.float32).to(cfg.data_device)\n",
    "b_test = torch.tensor(np.array([test_data[b] for b in events])).permute(1, 0).type(torch.float32).to(cfg.data_device)\n",
    "\n",
    "print(\"observations -\", y_train_obs.shape)\n",
    "print(\"labels -\", l_train.shape)\n",
    "print(\"events -\", b_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In some cases we may have alist of values of a parameter, i.e. movement_onset,\n",
    "where we have to preserve the correspondence bwtween this parameter and the train/valid/test datasets when shuffled.\n",
    "IMPORTANT: this means that the \"shuffle\" parameter in the data loader SHOULD ALWAYS BE SET TO \"False\", for all regimes.\n",
    "'''\n",
    "def sync_permutation(*tensors):\n",
    "    permutated = ()\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    permutation_indcs = torch.randperm(tensors[0].shape[0])\n",
    "    print(f\"permutation indices: {permutation_indcs}\")\n",
    "    for _, tensor in enumerate(tensors):\n",
    "        permutated += (tensor[permutation_indcs],)\n",
    "    lightning.seed_everything(cfg.seed, workers=True)\n",
    "    \n",
    "    return permutated\n",
    "\n",
    "if cfg.shuffle_train:\n",
    "    y_train_obs, l_train, b_train = sync_permutation(y_train_obs, l_train, b_train)\n",
    "if cfg.shuffle_valid:\n",
    "    y_valid_obs, l_valid, b_valid = sync_permutation(y_valid_obs, l_valid, b_valid)\n",
    "if cfg.shuffle_test:\n",
    "    y_test_obs, l_test, b_test = sync_permutation(y_test_obs, l_test, b_test)\n",
    "    \n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train_obs, l_train)\n",
    "y_val_dataset = torch.utils.data.TensorDataset(y_valid_obs, l_valid)\n",
    "y_test_dataset = torch.utils.data.TensorDataset(y_test_obs, l_test)\n",
    "\n",
    "# IMPORTANT:\"shuffle\" parameter in the data loader SHOULD ALWAYS BE SET TO \"False\", for all regimes.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    y_train_dataset,\n",
    "    batch_size=cfg.minibatch_sz if cfg.use_minibatching else cfg.batch_sz,\n",
    "    num_workers=4, pin_memory=True, shuffle=False\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    y_val_dataset,\n",
    "    batch_size=cfg.minibatch_sz if cfg.use_minibatching else cfg.batch_sz,\n",
    "    num_workers=4, pin_memory=True, shuffle=False\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    y_test_dataset,\n",
    "    batch_size=cfg.minibatch_sz if cfg.use_minibatching else cfg.batch_sz,\n",
    "    num_workers=4, pin_memory=True, shuffle=False\n",
    ") \n",
    "\n",
    "# Data dimensions\n",
    "n_train_trials, n_bins, n_neurons_obs = y_train_obs.shape\n",
    "n_valid_trials = y_valid_obs.shape[0]\n",
    "n_test_trials = y_test_obs.shape[0]\n",
    "\n",
    "# Append data-related attributes to the config Dictionary.\n",
    "cfg['n_bins'] = n_bins\n",
    "# Number of time bins used by the model to infere the latents.\n",
    "cfg['n_bins_enc'] = train_data['n_bins_enc']\n",
    "# Number of timesteps used by the model to to predict and unroll the latnt trajectories for n_bins - n_bins_bhv timesteps.\n",
    "cfg['n_bins_bhv'] = cfg.n_bins // 4\n",
    "\n",
    "cfg['n_neurons_obs'] = n_neurons_obs\n",
    "# Number of top most active neurons used by the model to infere the latents.\n",
    "\"\"\"TODO: Let the user specifiy the number of neurons - order and then clip.\"\"\"\n",
    "\"\"\"TODO: Remove the 'n_bins_bhv' attribute from the data saved by NeuroTask.\"\"\"\n",
    "\"\"\"TODO: Put clearer destingtion between n_neurons_obs and n_neurons_enc. Maybe just use n_neurons_enc in the rest of the code?.\"\"\"\n",
    "cfg['n_neurons_enc'] = n_neurons_obs\n",
    "\n",
    "cfg = Cfg(cfg)\n",
    "\n",
    "print(f\"# training trials: {n_train_trials}\")\n",
    "print(f\"# validation trials: {n_valid_trials}\")\n",
    "print(f\"# testing trials: {n_test_trials}\")\n",
    "print(f\"# neurons: {n_neurons_obs}\")\n",
    "print(f\"# time bins: {n_bins}\")\n",
    "print(f\"# time bins used for forcasting: {cfg.n_bins_bhv}\")\n",
    "print(f\"# predicted time bins: {n_bins - cfg.n_bins_bhv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_ex_trials = 2\n",
    "trials_inds = np.random.choice(range(0, y_valid_obs.shape[0]), size=n_ex_trials, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_ex_trials, figsize=(10, 8))\n",
    "fig.suptitle(f'{n_ex_trials} example trials')\n",
    "\n",
    "for trial_idx, trial in enumerate(trials_inds):\n",
    "    for neuron_idx in range(y_valid_obs.shape[2]):\n",
    "        \n",
    "        axes[trial_idx].set_title(f'\\n\\ntrial {trial+1}\\n', fontsize=10)\n",
    "        axes[trial_idx].scatter(\n",
    "            np.where(y_valid_obs[trial, :, neuron_idx].cpu() == 1)[0],\n",
    "            [neuron_idx] * len(np.where(y_valid_obs[trial, :, neuron_idx].cpu() == 1)[0]),\n",
    "            s=2, color='gray', marker='|'\n",
    "        )\n",
    "        \n",
    "axes[n_ex_trials-1].set_xlabel(f'time bin')\n",
    "axes[0].set_ylabel(f'neuron')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the structure of the state-space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"likelihood pdf\"\"\"\n",
    "H = utils.ReadoutLatentMask(cfg.n_latents, cfg.n_latents_read)\n",
    "readout_fn = nn.Sequential(H, nn.Linear(cfg.n_latents_read, n_neurons_obs))\n",
    "readout_fn[-1].bias.data = prob_utils.estimate_poisson_rate_bias(train_dataloader, cfg.bin_sz)\n",
    "likelihood_pdf = PoissonLikelihood(readout_fn, n_neurons_obs, cfg.bin_sz, device=cfg.device)\n",
    "\n",
    "\"\"\"dynamics module\"\"\"\n",
    "Q_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "dynamics_fn = utils.build_gru_dynamics_function(cfg.n_latents, cfg.n_hidden_dynamics, device=cfg.device)\n",
    "dynamics_mod = DenseGaussianDynamics(dynamics_fn, cfg.n_latents, Q_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"initial condition\"\"\"\n",
    "m_0 = torch.zeros(cfg.n_latents, device=cfg.device)\n",
    "Q_0_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "initial_condition_pdf = DenseGaussianInitialCondition(cfg.n_latents, m_0, Q_0_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"local/backward encoder\"\"\"\n",
    "backward_encoder = BackwardEncoderLRMvn(cfg.n_latents, cfg.n_hidden_backward, cfg.n_latents,\n",
    "                                        rank_local=cfg.rank_local, rank_backward=cfg.rank_backward,\n",
    "                                        device=cfg.device)\n",
    "local_encoder = LocalEncoderLRMvn(cfg.n_latents, n_neurons_obs, cfg.n_hidden_local, cfg.n_latents,\n",
    "                                  rank=cfg.rank_local,\n",
    "                                  device=cfg.device, dropout=cfg.p_local_dropout)\n",
    "nl_filter = NonlinearFilter(dynamics_mod, initial_condition_pdf, device=cfg.device)\n",
    "\n",
    "\"\"\"sequence vae\"\"\"\n",
    "ssm = LowRankNonlinearStateSpaceModel(dynamics_mod, likelihood_pdf, initial_condition_pdf, backward_encoder,\n",
    "                                      local_encoder, nl_filter, device=cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"lightning\"\"\"\n",
    "model_ckpt_path = 'ckpts/smoother/acausal/last-v11.ckpt'\n",
    "seq_vae = LightningMonkeyReaching.load_from_checkpoint(model_ckpt_path, ssm=ssm, cfg=cfg,\n",
    "                                                       n_time_bins_enc=cfg.n_bins_enc, n_time_bins_bhv=cfg.n_bins_bhv,\n",
    "                                                       strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_vae = LightningMonkeyReaching(ssm, cfg, cfg.n_bins_enc, cfg.n_bins_bhv)\n",
    "csv_logger = CSVLogger('logs/smoother/acausal/', name=f'sd_{cfg.seed}_r_y_{cfg.rank_local}_r_b_{cfg.rank_backward}', version='smoother_acausal')\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=3, monitor='r2_valid_enc', mode='max', dirpath='ckpts/smoother/acausal/', save_last=True,\n",
    "                                filename='{epoch:0}_{valid_loss:0.2f}_{r2_valid_enc:0.2f}_{r2_valid_bhv:0.2f}_{valid_bps_enc:0.2f}')\n",
    "\n",
    "# From epoch 0 to the end, accumulate the gradient for every batch_sz//minibatch_sz before the optimization step,\n",
    "# i.e. batch_sz//minibatch_sz forward passes, and then the backward pass, and so on.\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: cfg.batch_sz//cfg.minibatch_sz,})\n",
    "\n",
    "trainer = lightning.Trainer(max_epochs=cfg.n_epochs,\n",
    "                            #accelerator='gpu',\n",
    "                            #devices='auto',\n",
    "                            #strategy='ddp_notebook',\n",
    "                            #strategy='deepspeed_stage_3',\n",
    "                            #strategy='deepspeed_stage_3_offload', precision=16,\n",
    "                            gradient_clip_val=1.0,\n",
    "                            default_root_dir='lightning/',\n",
    "                            callbacks=[ckpt_callback, accumulator],\n",
    "                            logger=csv_logger,\n",
    "                            ) \n",
    "\n",
    "trainer.fit(model=seq_vae, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "torch.save(ckpt_callback.best_model_path, 'ckpts/smoother/acausal/best_model_path.pt')\n",
    "trainer.test(dataloaders=test_dataloader, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#seq_vae = LightningMonkeyReaching(ssm, cfg, cfg.n_bins_enc, cfg.n_bins_bhv)\n",
    "csv_logger = CSVLogger('logs/smoother/acausal/', name=f'sd_{cfg.seed}_r_y_{cfg.rank_local}_r_b_{cfg.rank_backward}', version='smoother_acausal')\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=3, monitor='r2_valid_enc', mode='max', dirpath='ckpts/smoother/acausal/', save_last=True,\n",
    "                                filename='{epoch:0}_{valid_loss:0.2f}_{r2_valid_enc:0.2f}_{r2_valid_bhv:0.2f}_{valid_bps_enc:0.2f}')\n",
    "\n",
    "trainer = lightning.Trainer(max_epochs=cfg.n_epochs,\n",
    "                            accelerator=cfg.device,\n",
    "                            #devices='auto',\n",
    "                            #strategy='ddp_notebook',\n",
    "                            #strategy='deepspeed_stage_3',\n",
    "                            #strategy='deepspeed_stage_3_offload', precision=16,\n",
    "                            gradient_clip_val=1.0,\n",
    "                            default_root_dir='lightning/',\n",
    "                            callbacks=[ckpt_callback],\n",
    "                            logger=csv_logger,\n",
    "                            ) \n",
    "\n",
    "trainer.fit(model=seq_vae, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "torch.save(ckpt_callback.best_model_path, 'ckpts/smoother/acausal/best_model_path.pt')\n",
    "trainer.test(dataloaders=test_dataloader, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"lightning\"\"\"\n",
    "model_ckpt_path = 'ckpts/smoother/acausal/last-v11.ckpt'\n",
    "seq_vae = LightningMonkeyReaching.load_from_checkpoint(model_ckpt_path, ssm=ssm, cfg=cfg,\n",
    "                                                       n_time_bins_enc=cfg.n_bins_enc, n_time_bins_bhv=cfg.n_bins_bhv,\n",
    "                                                       strict=False)\n",
    "\"\"\"extract trained ssm from lightning module\"\"\"\n",
    "seq_vae.ssm = seq_vae.ssm.to(cfg.device)\n",
    "seq_vae.ssm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.cuda import memory_allocated, memory_reserved, memory_stats\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if cfg.device == 'cuda':\n",
    "    print(\"Active GPU Tensors:\\n\")\n",
    "    for i, tensor in enumerate([obj for obj in gc.get_objects() if torch.is_tensor(obj) and obj.is_cuda]):\n",
    "        print(f\"Tensor {i+1}: size {tensor.size()}, dtype {tensor.dtype}, memory {tensor.element_size() * tensor.nelement() / 1e6} MB\")\n",
    "\n",
    "    print(f\"\\nTotal memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1e9} GB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved(0) / 1e9} GB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_cached(0) / 1e9} GB\")\n",
    "    print(f\"Free memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering the latent trajectories and using them to generate corresponding observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"inference\"\"\"\n",
    "z_s_train = []\n",
    "z_f_train = []\n",
    "z_p_train = []\n",
    "\n",
    "rates_train_s = []\n",
    "rates_train_f = []\n",
    "rates_train_p = []\n",
    "\n",
    "z_s_valid = []\n",
    "z_f_valid = []\n",
    "z_p_valid = []\n",
    "\n",
    "rates_valid_s = []\n",
    "rates_valid_f = []\n",
    "rates_valid_p = []\n",
    "\n",
    "z_s_test = []\n",
    "z_f_test = []\n",
    "z_p_test = []\n",
    "\n",
    "rates_test_s = []\n",
    "rates_test_f = []\n",
    "rates_test_p = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    m_0 = seq_vae.ssm.nl_filter.initial_c_pdf.m_0\n",
    "    Q_0 = Fn.softplus(seq_vae.ssm.nl_filter.initial_c_pdf.log_Q_0)\n",
    "    #m_0 = seq_vae.ssm.dynamics_mod.initial_c_pdf.m_0\n",
    "    #Q_0 = Fn.softplus(seq_vae.ssm.dynamics_mod.initial_c_pdf.log_Q_0)\n",
    "\n",
    "    z_ic = m_0 + Q_0.sqrt() * torch.randn([n_test_trials, n_neurons_obs] + [cfg.n_latents], device=cfg.device)\n",
    "    #z_ic_p = seq_vae.ssm.predict_forward(z_ic, cfg.n_bins - cfg.n_bins_bhv)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"train dataloader\"):\n",
    "        \n",
    "        #print(f\"\\ntrain batch: {batch_idx + 1}\")\n",
    "        #print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9} GB   Reserved: {torch.cuda.memory_reserved(0) / 1e9} GB   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9} GB\")\n",
    "        \n",
    "        loss, z_s, stats = seq_vae.ssm(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], cfg.n_bins - cfg.n_bins_bhv)\n",
    "        z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "        \n",
    "        rates_s = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s)).mean(dim=0).to('cpu')\n",
    "        rates_f = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f)).mean(dim=0).to('cpu')\n",
    "        rates_p = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p)).mean(dim=0).to('cpu')\n",
    "        \n",
    "        z_s_train.append(z_s.to('cpu'))\n",
    "        z_f_train.append(z_f.to('cpu'))\n",
    "        z_p_train.append(z_p.to('cpu'))\n",
    "        \n",
    "        rates_train_s.append(rates_s)\n",
    "        rates_train_f.append(rates_f)\n",
    "        rates_train_p.append(rates_f)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(valid_dataloader), total=len(test_dataloader), desc=\"valid dataloader\"):\n",
    "        \n",
    "        #print(f\"\\nvalid batch: {batch_idx + 1}\")\n",
    "        #print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9} GB   Reserved: {torch.cuda.memory_reserved(0) / 1e9} GB   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9} GB\")\n",
    "        \n",
    "        loss, z_s, stats = seq_vae.ssm(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], cfg.n_bins - cfg.n_bins_bhv)\n",
    "        z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "        \n",
    "        rates_s = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s)).mean(dim=0).to('cpu')\n",
    "        rates_f = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f)).mean(dim=0).to('cpu')\n",
    "        rates_p = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p)).mean(dim=0).to('cpu')\n",
    "        \n",
    "        z_s_valid.append(z_s.to('cpu'))\n",
    "        z_f_valid.append(z_f.to('cpu'))\n",
    "        z_p_valid.append(z_p.to('cpu'))\n",
    "        \n",
    "        rates_valid_s.append(rates_s)\n",
    "        rates_valid_f.append(rates_f)\n",
    "        rates_valid_p.append(rates_p)\n",
    "        \n",
    "    for batch_idx, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"test dataloader\"):\n",
    "        \n",
    "        #print(f\"\\ntest batch: {batch_idx + 1}\")\n",
    "        #print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9} GB   Reserved: {torch.cuda.memory_reserved(0) / 1e9} GB   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9} GB\")\n",
    "        \n",
    "        loss, z_s, stats = seq_vae.ssm(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0].to(cfg.device), cfg.n_samples)\n",
    "        z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], cfg.n_bins - cfg.n_bins_bhv)\n",
    "        z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "        \n",
    "        rates_s = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s)).mean(dim=0).to('cpu')\n",
    "        rates_f = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f)).mean(dim=0).to('cpu')\n",
    "        rates_p = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p)).mean(dim=0).to('cpu')\n",
    "        \n",
    "        z_s_test.append(z_s.to('cpu'))\n",
    "        z_f_test.append(z_f.to('cpu'))\n",
    "        z_p_test.append(z_p.to('cpu'))\n",
    "        \n",
    "        rates_test_s.append(rates_s)\n",
    "        rates_test_f.append(rates_f)\n",
    "        rates_test_p.append(rates_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_s_train = torch.cat(z_s_train, dim=1).cpu()\n",
    "z_f_train = torch.cat(z_f_train, dim=1).cpu()\n",
    "z_p_train = torch.cat(z_p_train, dim=1).cpu()\n",
    "\n",
    "rates_train_s = torch.cat(rates_train_s, dim=0).cpu()\n",
    "rates_train_f = torch.cat(rates_train_f, dim=0).cpu()\n",
    "rates_train_p = torch.cat(rates_train_p, dim=0).cpu()\n",
    "\n",
    "z_s_valid = torch.cat(z_s_valid, dim=1).cpu()\n",
    "z_f_valid = torch.cat(z_f_valid, dim=1).cpu()\n",
    "z_p_valid = torch.cat(z_p_valid, dim=1).cpu()\n",
    "\n",
    "rates_valid_s = torch.cat(rates_valid_s, dim=0).cpu()\n",
    "rates_valid_f = torch.cat(rates_valid_f, dim=0).cpu()\n",
    "rates_valid_p = torch.cat(rates_valid_p, dim=0).cpu()\n",
    "\n",
    "z_s_test = torch.cat(z_s_test, dim=1).cpu()\n",
    "z_f_test = torch.cat(z_f_test, dim=1).cpu()\n",
    "z_p_test = torch.cat(z_p_test, dim=1).cpu()\n",
    "\n",
    "rates_test_s = torch.cat(rates_test_s, dim=0).cpu()\n",
    "rates_test_f = torch.cat(rates_test_f, dim=0).cpu()\n",
    "rates_test_p = torch.cat(rates_test_p, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "spikes per time bin.\n",
    "\n",
    "To get the number of spikes within each time bin, we should do the poisson sampling from the firing rate as\n",
    "the number of spikes per time bin.\n",
    "'''\n",
    "\n",
    "spikes_train_s = torch.poisson(cfg.bin_sz * rates_train_s).cpu()\n",
    "spikes_train_f = torch.poisson(cfg.bin_sz * rates_train_f).cpu()\n",
    "spikes_train_p = torch.poisson(cfg.bin_sz * rates_train_p).cpu()\n",
    "\n",
    "spikes_test_s = torch.poisson(cfg.bin_sz * rates_test_s).cpu()\n",
    "spikes_test_f = torch.poisson(cfg.bin_sz * rates_test_f).cpu()\n",
    "spikes_test_p = torch.poisson(cfg.bin_sz * rates_test_p).cpu()\n",
    "\n",
    "spikes_valid_s = torch.poisson(cfg.bin_sz * rates_valid_s).cpu()\n",
    "spikes_valid_f = torch.poisson(cfg.bin_sz * rates_valid_f).cpu()\n",
    "spikes_valid_p = torch.poisson(cfg.bin_sz * rates_valid_p).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"colors\"\"\"\n",
    "blues = cm.get_cmap(\"winter\", z_s_test.shape[0])\n",
    "reds = cm.get_cmap(\"summer\", z_s_test.shape[0])\n",
    "springs = cm.get_cmap(\"spring\", z_s_test.shape[0])\n",
    "\n",
    "color_map_list = [blues, reds, springs]\n",
    "\n",
    "#n_ex_trials = 4\n",
    "#trial_indcs = np.random.choice(range(0, y_test_obs.shape[0]), size=n_ex_trials, replace=False)\n",
    "\n",
    "events_str = ['go cue', 'move\\nstarts', 'move\\nends']\n",
    "event_c = ['purple', 'black', 'coral']\n",
    "\n",
    "def plot_z_samples(fig, axs, latents, bhv_bins, trial_indcs, cfg, color_map_list, regime=None):\n",
    "    \n",
    "    samples = latents[:, trial_indcs, ..., :3]\n",
    "    n_samples, n_trials, n_bins, n_neurons = samples.shape\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    \n",
    "    if regime == 'prediction':\n",
    "        [axs[i].axvline(cfg.n_bins_bhv, linestyle='--', color='red') for i in range(n_trials)]\n",
    "    for bi in range(bhv_bins.shape[1]):\n",
    "        [axs[i].axvline(bhv_bins[trial_indcs[i], bi], linestyle='--', color=event_c[bi]) for i in range(n_trials)]\n",
    "    \n",
    "    [axs[i].axis('off') for i in range(n_trials-1)]\n",
    "    axs[-1].yaxis.set_visible(False)\n",
    "    axs[-1].spines['left'].set_visible(False)\n",
    "    axs[-1].spines['right'].set_visible(False)\n",
    "    axs[-1].spines['top'].set_visible(False)\n",
    "    \n",
    "    [axs[i].plot(samples[j, i, :, n], color=color_map_list[n](j), linewidth=0.5, alpha=0.4)\n",
    "     for i in range(n_trials) for j in range(samples.shape[0]) for n in range(n_neurons)]\n",
    "    \n",
    "    [axs[i].set_title(f'trial {trial_indcs[i]+1}', fontsize=7) for i in range(n_trials)]\n",
    "    [axs[i].set_xlim(0, n_bins) for i in range(n_trials)]\n",
    "\n",
    "\n",
    "def mark_events(cfg, bhv_bins, trial_indc, regime=None):\n",
    "    y_min, _ = plt.ylim()\n",
    "    \n",
    "    if regime == 'prediction':\n",
    "        plt.annotate('pred\\nstarts', xy=(cfg.n_bins_bhv, y_min), xytext=(cfg.n_bins_bhv + 0.25 * cfg.n_bins_bhv, y_min + 0.9 * y_min),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8, ha='center'\n",
    "                )\n",
    "    for bi in range(bhv_bins.shape[1]):\n",
    "        e = bhv_bins[trial_indcs[-1], bi]\n",
    "        bhv = events_str[bi]\n",
    "\n",
    "        plt.annotate(bhv, xy=(e, y_min), xytext=(e + 0.25 * cfg.n_bins_bhv, y_min + 0.9 * y_min),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                     fontsize=8, ha='center'\n",
    "                    )\n",
    "\n",
    "        \n",
    "\"\"\"filtered\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_z_samples(fig, axs, z_f_test.cpu(), b_test, trial_indcs, cfg, color_map_list)\n",
    "mark_events(cfg, b_test, trial_indcs)\n",
    "fig.suptitle(f'\\nLatent trajectories 1, 2 and 3 ({z_s_test.shape[0]} samples each) - inferred by using the filtering distribution of z given y_hat\\ndirectly in the dynamics function of the SSM\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/25_z_f_samples_maze_5ms.png\")\n",
    "plt.show()\n",
    "    \n",
    "\"\"\"smoothed\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_z_samples(fig, axs, z_s_test.cpu(), b_test, trial_indcs, cfg, color_map_list)\n",
    "mark_events(cfg, b_test, trial_indcs)\n",
    "fig.suptitle(f'\\nLatent trajectories 1, 2 and 3 ({z_s_test.shape[0]} samples each) - inferred by using the filtering distribution\\nto approximate the smoothing distribution of z given the real data\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/25_z_s_samples_maze_5ms.png\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"predicted\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_z_samples(fig, axs, z_p_test.cpu(), b_test, trial_indcs, cfg, color_map_list, regime='prediction')\n",
    "mark_events(cfg, b_test, trial_indcs, regime='prediction')\n",
    "fig.suptitle(f'\\nUnrolled latent trajectories 1, 2, and 3 ({z_s_test.shape[0]} samples each) for {cfg.n_bins - cfg.n_bins_bhv} time bins\\nby predicting from the first {cfg.n_bins_bhv} bins of the smoothed latents\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/25_z_p_samples_maze_5ms.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = ['gold', 'navy', 'coral']\n",
    "\n",
    "#n_ex_trials = 4\n",
    "#trial_indcs = np.random.choice(range(0, y_test_obs.shape[0]), size=n_ex_trials, replace=False)\n",
    "\n",
    "events_str = ['go cue', 'move\\nstarts', 'move\\nends']\n",
    "event_c = ['purple', 'black', 'green']\n",
    "\n",
    "def plot_avg_z_samples(fig, axs, latents, bhv_bins, trial_indcs, cfg, colors=None, regime=None):\n",
    "    \n",
    "    samples = latents[:, trial_indcs, ..., :3]\n",
    "    n_samples, n_trials, n_bins, n_latents = samples.shape\n",
    "\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    \n",
    "    if regime == 'prediction':\n",
    "        [axs[i].axvline(cfg.n_bins_bhv, linestyle='--', color='red') for i in range(n_trials)]\n",
    "    for bi in range(bhv_bins.shape[1]):\n",
    "        [axs[i].axvline(bhv_bins[trial_indcs[i], bi], linestyle='--', color=event_c[bi]) for i in range(n_trials)]\n",
    "    \n",
    "    [axs[i].axis('off') for i in range(n_trials-1)]\n",
    "    axs[-1].yaxis.set_visible(False)\n",
    "    axs[-1].spines['left'].set_visible(False)\n",
    "    axs[-1].spines['right'].set_visible(False)\n",
    "    axs[-1].spines['top'].set_visible(False)\n",
    "    \n",
    "    [axs[i].plot(samples[0, i, :, n], color=colors[n], linewidth=1.0, alpha=0.8)\n",
    "     for i in range(n_trials) for n in range(n_latents)]\n",
    "    \n",
    "    [axs[i].set_title(f'trial {trial_indcs[i]+1}', fontsize=7) for i in range(n_trials)]\n",
    "    [axs[i].set_xlim(0, n_bins) for i in range(n_trials)]\n",
    "\n",
    "        \n",
    "\"\"\"filtered\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_avg_z_samples(fig, axs, z_p_test.mean(dim=0, keepdim=True).cpu(), b_test, trial_indcs, cfg, colors, regime=None)\n",
    "mark_events(cfg, b_test, trial_indcs)\n",
    "fig.suptitle(f'\\nLatent trajectories 1, 2 and 3 - inferred by using the filtering distribution of z given y_hat\\ndirectly in the dynamics function of the SSM\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/avg_z_f_samples_maze_5ms.png\")\n",
    "plt.show()\n",
    "    \n",
    "\"\"\"smoothed\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_avg_z_samples(fig, axs, z_p_test.mean(dim=0, keepdim=True).cpu(), b_test, trial_indcs, cfg, colors, regime=None)\n",
    "mark_events(cfg, b_test, trial_indcs)\n",
    "fig.suptitle(f'\\nLatent trajectories 1, 2 and 3 - inferred by using the filtering distribution\\nto approximate the smoothing distribution of z given the real data\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/avg_z_s_samples_maze_5ms.png\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"predicted\"\"\"\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_avg_z_samples(fig, axs, z_p_test.mean(dim=0, keepdim=True).cpu(), b_test, trial_indcs, cfg, colors, regime='prediction')\n",
    "mark_events(cfg, b_test, trial_indcs, regime='prediction')\n",
    "fig.suptitle(f'\\nUnrolled latent trajectories 1, 2, and 3 for {cfg.n_bins - cfg.n_bins_bhv} time bins by predicting\\nfrom the first {cfg.n_bins_bhv} bins of the smoothed latents\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/avg_z_p_samples_maze_5ms.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#n_ex_trials = 4\n",
    "#trial_indcs = np.random.choice(range(0, y_test_obs.shape[0]), size=n_ex_trials, replace=False)\n",
    "\n",
    "events_str = ['go cue', 'move\\nstarts', 'move\\nends']\n",
    "event_c = ['purple', 'black', 'green']\n",
    "\n",
    "def plot_f_vs_p(fig, axs, z_f_z_p, bhv_bins, trial_indcs, cfg, colors=None, regimes=None):\n",
    "    \n",
    "    samples = z_f_z_p[0][:, trial_indcs, ..., :3]\n",
    "    n_samples, n_trials, n_bins, n_latents = samples.shape\n",
    "\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    \n",
    "    [axs[i].axvline(cfg.n_bins_bhv, linestyle='--', color='red') for i in range(n_trials)]\n",
    "    for bi in range(bhv_bins.shape[1]):\n",
    "        [axs[i].axvline(bhv_bins[trial_indcs[i], bi], linestyle='--', color=event_c[bi]) for i in range(n_trials)]\n",
    "    \n",
    "    [axs[i].axis('off') for i in range(n_trials-1)]\n",
    "    axs[-1].yaxis.set_visible(False)\n",
    "    axs[-1].spines['left'].set_visible(False)\n",
    "    axs[-1].spines['right'].set_visible(False)\n",
    "    axs[-1].spines['top'].set_visible(False)\n",
    "    \n",
    "    for ir, regime in enumerate(regimes):\n",
    "        [axs[i].plot(z_f_z_p[ir][:, i, :, n].mean(dim=0), color=colors[ir], linewidth=1.0, alpha=0.8, label=regime if ir==0 and i==0 and n==0 else '')\n",
    "         for i in range(n_trials) for n in range(n_latents)]\n",
    "    \n",
    "    [axs[i].set_title(f'trial {trial_indcs[i]+1}', fontsize=7) for i in range(n_trials)]\n",
    "    [axs[i].set_xlim(0, n_bins) for i in range(n_trials)]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(8, 8))\n",
    "plot_f_vs_p(fig, axs, [z_f_test, z_p_test], b_test, trial_indcs, cfg, colors=['purple', 'gray'], regimes=['filtering', 'prediction'])\n",
    "mark_events(cfg, b_test, trial_indcs, regime='prediction')\n",
    "fig.suptitle(f'\\nUnrolled latent trajectories 1, 2, and 3 for {cfg.n_bins - cfg.n_bins_bhv} time bins by predicting\\nfrom the first {cfg.n_bins_bhv} bins of the smoothed latents\\n')\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"output_figs/f_vs_p.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed trials vs observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#n_ex_trials = 4\n",
    "#trial_indcs = np.random.choice(range(0, spikes_valid_s.shape[0]), size=n_ex_trials, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=n_ex_trials, figsize=(18, 12))\n",
    "fig.suptitle(f'{n_ex_trials} example trials')\n",
    "\n",
    "for trial_idx, trial in enumerate(trial_indcs):\n",
    "    for neuron_idx in range(spikes_valid_s.shape[2]):\n",
    "        \n",
    "        axes[trial_idx, 0].scatter(\n",
    "            np.where(y_valid_obs[trial, :, neuron_idx] == 1)[0],\n",
    "            [neuron_idx] * len(np.where(y_valid_obs[trial, :, neuron_idx] == 1)[0]),\n",
    "            s=1, color='gray', marker='|'\n",
    "        )\n",
    "        axes[trial_idx, 1].scatter(\n",
    "            np.where(spikes_valid_s[trial, :, neuron_idx] == 1)[0],\n",
    "            [neuron_idx] * len(np.where(spikes_valid_s[trial, :, neuron_idx] == 1)[0]),\n",
    "            s=1, color='gray', marker='|'\n",
    "        )\n",
    "    axes[trial_idx, 0].axvline(b_test[trial, 0], linestyle='--', alpha=1.0, linewidth=1.5, color='purple')\n",
    "    axes[trial_idx, 1].axvline(b_test[trial, 0], linestyle='--', alpha=1.0, linewidth=1.5, color='purple')\n",
    "    axes[trial_idx, 0].axvline(b_test[trial, 2], linestyle='--', alpha=1.0, linewidth=1.5, color='coral')\n",
    "    axes[trial_idx, 1].axvline(b_test[trial, 2], linestyle='--', alpha=1.0, linewidth=1.5, color='coral')\n",
    "        \n",
    "[ax.axvline(b_test[trial_indcs[-1], 1], linestyle='--', color='black') for ax in axes.flat]\n",
    "[ax.axvline(b_test[trial_indcs[-1], 1], linestyle='--', color='black') for ax in axes.flat]\n",
    "        \n",
    "plt.annotate('go_cue', xy=(b_test[trial_indcs[-1], 0], 0), xytext=(b_test[trial_indcs[-1], 0], -0.4*n_neurons_obs),\n",
    "     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "     fontsize=8, ha='center')\n",
    "\n",
    "plt.annotate('move\\nstarts', xy=(b_test[trial_indcs[-1], 1], 0), xytext=(b_test[trial_indcs[-1], 1], -0.4*n_neurons_obs),\n",
    "     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "     fontsize=8, ha='center')\n",
    "\n",
    "plt.annotate('move\\nends', xy=(b_test[trial_indcs[-1], 2], 0), xytext=(b_test[trial_indcs[-1], 2], -0.4*n_neurons_obs),\n",
    "     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "     fontsize=8, ha='center')\n",
    "        \n",
    "axes[0, 0].set_title('real\\n')\n",
    "axes[0, 1].set_title('generated\\n')\n",
    "        \n",
    "axes[len(trial_indcs)-1, 0].set_xlabel(f'time bin')\n",
    "axes[0, 0].set_ylabel(f'neuron')\n",
    "\n",
    "plt.savefig(\"output_figs/reconstructed_vs_real.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_spike_count = torch.sum(y_test_obs, axis=1)\n",
    "model_spike_count = torch.sum(spikes_test_s, axis=1)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "fig.suptitle('Total spike count of each neuron in each timestep in all trials')\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    axs[0].set_title('real')\n",
    "    dsc = axs[0].imshow(data_spike_count, interpolation='none', aspect='auto')\n",
    "    axs[1].set_title('generated')\n",
    "    msc = axs[1].imshow(model_spike_count, interpolation='none', aspect='auto')\n",
    "\n",
    "    vmin, _ = min(torch.min(data_spike_count.flatten(), dim=0), torch.min(model_spike_count.flatten(), dim=0))\n",
    "    vmax, _ = max(torch.max(data_spike_count.flatten(), dim=0), torch.max(model_spike_count.flatten(), dim=0))\n",
    "\n",
    "    fig.colorbar(dsc, ax=axs[0], shrink=0.4).mappable.set_clim(vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(msc, ax=axs[1], shrink=0.4).mappable.set_clim(vmin=vmin, vmax=vmax)\n",
    "                \n",
    "axs[1].set_xlabel('neuron')\n",
    "axs[0].set_ylabel('trial')\n",
    "\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"output_figs/spike_count.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial-averaged neurons activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_events_on_psth(ax, cfg, bhv_bins):\n",
    "    _, y_upper_limit = ax.get_ylim()\n",
    "    \n",
    "    for bi in range(bhv_bins.shape[1]):\n",
    "        bhv_bin = int(bhv_bins[:, bi].mean(dim=0))\n",
    "        ev = events_str[bi]\n",
    "\n",
    "        ax.annotate(f\"avg {ev}\",\n",
    "                    xy=(bhv_bin * cfg.bin_sz_ms, y_upper_limit),\n",
    "                    xytext=(bhv_bin * cfg.bin_sz_ms - (cfg.n_bins * 0.4), (y_upper_limit * 1.4)),\n",
    "                    arrowprops=dict(facecolor='black', alpha=0.2, arrowstyle='->'),\n",
    "                    fontsize=7, alpha=0.8, ha='center')\n",
    "        \n",
    "        \n",
    "n_neurons_to_plot = 16\n",
    "neuron_indcs = np.random.choice(range(0, y_valid_obs.shape[2]), size=n_neurons_to_plot, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(int(np.sqrt(n_neurons_to_plot)), int(np.sqrt(n_neurons_to_plot)), figsize=(14, 10))\n",
    "fig.suptitle(f'Trial-averaged neurons activity\\n\\n\\n')\n",
    "\n",
    "for ax, neuron in zip(axes.flat, neuron_indcs):\n",
    "\n",
    "    fr_data = torch.tensor(\n",
    "        # Here we divide by the bin size to get the number of spikes per second, not per bin.\n",
    "        torch.mean(y_valid_obs[:, :, neuron], axis=0) / cfg.bin_sz,\n",
    "    )\n",
    "\n",
    "    fr_model= torch.tensor(\n",
    "        # Average over trials.\n",
    "        torch.mean(rates_test_s[:, :, neuron], axis=0),\n",
    "    )\n",
    "\n",
    "    ax.plot(np.arange(n_bins) * cfg.bin_sz_ms, fr_data, color= 'black', alpha=0.8, label='data' if neuron == neuron_indcs[-1] else '')\n",
    "    ax.plot(np.arange(n_bins) * cfg.bin_sz_ms, fr_model, color= 'red', alpha=0.6, label='model' if neuron == neuron_indcs[-1] else '')\n",
    "    \n",
    "    ax.axvline(x=b_test[:, 0].mean(dim=0) * cfg.bin_sz_ms, color='coral', linestyle='--')\n",
    "    ax.axvline(x=b_test[:, 1].mean(dim=0) * cfg.bin_sz_ms, color='navy', linestyle='--')\n",
    "    ax.axvline(x=b_test[:, 2].mean(dim=0) * cfg.bin_sz_ms, color='gold', linestyle='--')\n",
    "\n",
    "    ax.set_title(f'\\nneuron {neuron+1}\\n', fontsize=8)\n",
    "    ax.set_xlabel('time (ms)' if neuron == neuron_indcs[-int(np.sqrt(n_neurons_to_plot))] else '', fontsize=9)\n",
    "    ax.set_ylabel('firing rate' if neuron == neuron_indcs[0] else '', fontsize=9)\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "    if neuron == neuron_indcs[0]:\n",
    "        mark_events_on_psth(ax, cfg, b_test)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=1, fontsize=8)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"output_figs/trial_averaged_activity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement preparation and movement executation time\n",
    "Check this out, it's cool, if you are using data from a motor task with a go cue, therefor, also a time point for initiating the movement, and, if you are lucky, a time point for terminating the movement.\n",
    "\n",
    "This could be used to predict the reaction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_move_time(bhv_bins):\n",
    "    prep_t = bhv_bins[:, 1] - bhv_bins[:, 0]\n",
    "    move_t = bhv_bins[:, 2] - bhv_bins[:, 1]\n",
    "    \n",
    "    return prep_t, move_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pt, train_mt = get_move_time(b_train)\n",
    "valid_pt, valid_mt = get_move_time(b_valid)\n",
    "test_pt, test_mt = get_move_time(b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vel_train = l_train\n",
    "vel_valid = l_valid\n",
    "vel_test = l_test\n",
    "\n",
    "vel = torch.concat([vel_train, vel_valid, vel_test], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_spikes = torch.concat([y_train_obs, y_valid_obs, y_test_obs], dim=0)\n",
    "model_spikes = torch.concat([spikes_train_s, spikes_valid_s, spikes_test_s], dim=0)\n",
    "\n",
    "data_rates = data_spikes / cfg.bin_sz\n",
    "model_rates = torch.concat([rates_train_s, rates_valid_s, rates_test_s], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_train = torch.tensor(np.array([train_data[l] for l in ['cursor_pos_x', 'cursor_pos_x']])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)\n",
    "if cfg.shuffle_train:\n",
    "    pos_train = sync_permutation(pos_train)[0]\n",
    "pos_valid = torch.tensor(np.array([val_data[l] for l in ['cursor_pos_x', 'cursor_pos_x']])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)\n",
    "pos_test = torch.tensor(np.array([test_data[l] for l in ['cursor_pos_x', 'cursor_pos_x']])).permute(1, 2, 0).type(torch.float32).to(cfg.data_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note that the decoder is trained on the rates generated by XFADS from the latents inferred from the training data.\n",
    "But tested on the rates generated from the latents inferred from the validation and the testing data.\n",
    "'''\n",
    "\n",
    "clf = Ridge(alpha=0.01)\n",
    "# fit to training data\n",
    "clf.fit(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    r2 = clf.score(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    \n",
    "    r2_train = clf.score(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    r2_valid = clf.score(rates_valid_s.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    r2_test = clf.score(rates_test_s.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    \n",
    "    r2_filter = clf.score(rates_test_f.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    r2_k_step = []\n",
    "    \n",
    "    # transform train data\n",
    "    r2_train_s = clf.score(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    r2_train_f = clf.score(rates_train_f.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    r2_train_p = clf.score(rates_train_p.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    vel_hat_train_s = clf.predict(rates_train_s.reshape(-1, n_neurons_obs)).reshape(list(y_train_obs.shape)[:-1] + [vel_train.shape[-1]])\n",
    "    vel_hat_train_f = clf.predict(rates_train_f.reshape(-1, n_neurons_obs)).reshape(list(y_train_obs.shape)[:-1] + [vel_train.shape[-1]])\n",
    "    vel_hat_train_p = clf.predict(rates_train_p.reshape(-1, n_neurons_obs)).reshape(list(y_train_obs.shape)[:-1] + [vel_train.shape[-1]])\n",
    "    \n",
    "    # transform valid data\n",
    "    r2_valid_s = clf.score(rates_valid_s.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    r2_valid_f = clf.score(rates_valid_f.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    r2_valid_p = clf.score(rates_valid_p.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    vel_hat_valid_s = clf.predict(rates_valid_s.reshape(-1, n_neurons_obs)).reshape(list(y_valid_obs.shape)[:-1] + [vel_valid.shape[-1]])\n",
    "    vel_hat_valid_f = clf.predict(rates_valid_f.reshape(-1, n_neurons_obs)).reshape(list(y_valid_obs.shape)[:-1] + [vel_valid.shape[-1]])\n",
    "    vel_hat_valid_p = clf.predict(rates_valid_p.reshape(-1, n_neurons_obs)).reshape(list(y_valid_obs.shape)[:-1] + [vel_valid.shape[-1]])\n",
    "    \n",
    "    # transform test data\n",
    "    r2_test_s = clf.score(rates_test_s.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    r2_test_f = clf.score(rates_test_f.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    r2_test_p = clf.score(rates_test_p.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    vel_hat_test_s = clf.predict(rates_test_s.reshape(-1, n_neurons_obs)).reshape(list(y_test_obs.shape)[:-1] + [vel_test.shape[-1]])\n",
    "    vel_hat_test_f = clf.predict(rates_test_f.reshape(-1, n_neurons_obs)).reshape(list(y_test_obs.shape)[:-1] + [vel_test.shape[-1]])\n",
    "    vel_hat_test_p = clf.predict(rates_test_p.reshape(-1, n_neurons_obs)).reshape(list(y_test_obs.shape)[:-1] + [vel_test.shape[-1]])\n",
    "    \n",
    "    for k in range(n_bins - cfg.n_bins_bhv):\n",
    "        z_prd_test = utils.propagate_latent_k_steps(z_f_valid[:, :, k], dynamics_mod.to('cpu'), n_bins + 0 - (k + 1))\n",
    "        z_prd_test = torch.concat([z_f_valid[:, :, :k], z_prd_test], dim=2)\n",
    "\n",
    "        # m_prd_test = z_prd_test.mean(dim=0)\n",
    "        # m_prd_test = torch.concat([m_filter[:, :k], m_prd_test[:, k:]], dim=1)\n",
    "\n",
    "        rates_prd_test = torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_prd_test.to(cfg.device))).mean(dim=0)\n",
    "\n",
    "        r2_prd = clf.score(rates_prd_test.reshape(-1, n_neurons_obs).cpu(), vel_valid.reshape(-1, 2))\n",
    "        r2_k_step.append(r2_prd)\n",
    "        \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering reaching trajectories\n",
    "If there is not enough metadata for finding the trial condirions, we can use a clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vel_to_pos = lambda v: torch.cumsum(torch.tensor(v), dim=1)\n",
    "\n",
    "pos_train = vel_to_pos(vel_train)\n",
    "pos_valid = vel_to_pos(vel_valid)\n",
    "pos_test = vel_to_pos(vel_test)\n",
    "\n",
    "pos_train_hat_s = vel_to_pos(vel_hat_train_s)\n",
    "pos_train_hat_f = vel_to_pos(vel_hat_train_f)\n",
    "pos_train_hat_p = vel_to_pos(vel_hat_train_p)\n",
    "\n",
    "pos_valid_hat_s = vel_to_pos(vel_hat_valid_s)\n",
    "pos_valid_hat_f = vel_to_pos(vel_hat_valid_f)\n",
    "pos_valid_hat_p = vel_to_pos(vel_hat_valid_p)\n",
    "\n",
    "pos_test_hat_s = vel_to_pos(vel_hat_test_s)\n",
    "pos_test_hat_f = vel_to_pos(vel_hat_test_f)\n",
    "pos_test_hat_p = vel_to_pos(vel_hat_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = torch.concat([pos_train, pos_valid, pos_test], dim=0)\n",
    "pos_hat_s = torch.concat([pos_train_hat_s, pos_valid_hat_s, pos_test_hat_s], dim=0)\n",
    "pos_hat_f = torch.concat([pos_train_hat_f, pos_valid_hat_f, pos_test_hat_f], dim=0)\n",
    "pos_hat_p = torch.concat([pos_train_hat_p, pos_valid_hat_p, pos_test_hat_p], dim=0)\n",
    "\n",
    "reach_angles_rad = torch.atan2(pos[:, -1, 0], pos[:, -1, 1])\n",
    "reach_angles_deg = torch.rad2deg(reach_angles_rad) + 360 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"plotting\"\"\"\n",
    "n_trials_plot = 125\n",
    "n_samples_mu_plt = 20\n",
    "\n",
    "blues = cm.get_cmap(\"Blues\", n_samples_mu_plt)\n",
    "grays = cm.get_cmap(\"Greys\", n_samples_mu_plt)\n",
    "yellows = cm.get_cmap(\"YlOrBr\", n_samples_mu_plt)\n",
    "\n",
    "trial_plt_dx = torch.randperm(n_test_trials)[:n_trials_plot]\n",
    "reach_angle = torch.atan2(pos_test[:, -1, 0], pos_test[:, -1, 1])\n",
    "reach_colors = plt.cm.hsv(reach_angle / (2 * np.pi) + 0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    plot_utils.plot_reaching(axs[0], pos_test[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[1], pos_test_hat_s[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[2], pos_test_hat_f[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[3], pos_test_hat_p[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "\n",
    "    axs[0].set_title('true')\n",
    "    axs[1].set_title(f'smoothed, r2:{r2_test_s:.3f}')\n",
    "    axs[2].set_title(f'filtered, r2:{r2_test_f:.3f}')\n",
    "    axs[3].set_title(f'predicted, r2:{r2_test_p:.3f}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"plotting\"\"\"\n",
    "n_trials_plot = 125\n",
    "n_samples_mu_plt = 20\n",
    "\n",
    "blues = cm.get_cmap(\"Blues\", n_samples_mu_plt)\n",
    "grays = cm.get_cmap(\"Greys\", n_samples_mu_plt)\n",
    "yellows = cm.get_cmap(\"YlOrBr\", n_samples_mu_plt)\n",
    "\n",
    "trial_plt_dx = torch.randperm(n_train_trials)[:n_trials_plot]\n",
    "reach_angle = torch.atan2(pos_train[:, -1, 0], pos_train[:, -1, 1])\n",
    "reach_colors = plt.cm.hsv(reach_angle / (2 * np.pi) + 0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    plot_utils.plot_reaching(axs[0], pos_train[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[1], pos_train_hat_s[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[2], pos_train_hat_f[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[3], pos_train_hat_p[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "\n",
    "    axs[0].set_title('true')\n",
    "    axs[1].set_title(f'smoothed, r2:{r2_train_s:.3f}')\n",
    "    axs[2].set_title(f'filtered, r2:{r2_train_f:.3f}')\n",
    "    axs[3].set_title(f'predicted, r2:{r2_train_p:.3f}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.axvline(12, linestyle='--')\n",
    "plt.plot(r2_k_step)\n",
    "plt.axhline(r2_train, color='green', label='smoothed')\n",
    "plt.axhline(r2_filter, color='orange', label='filtered')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reaching_dirs(angles):\n",
    "    \n",
    "    def get_reaching_dir(angle):\n",
    "        \n",
    "        if 0 <= angle < 90:\n",
    "            return 0\n",
    "        elif 90 <= angle < 180:\n",
    "            return 1\n",
    "        elif 180 <= angle <= 360:\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(\"Angle out of range\")\n",
    "\n",
    "    return torch.tensor([get_reaching_dir(angle) for angle in angles])\n",
    "\n",
    "\n",
    "directions = [0, 1, 2]\n",
    "reach_dirs = get_reaching_dirs(reach_angles_deg)\n",
    "\n",
    "\"\"\"def get_reaching_dirs(angles):\n",
    "    \n",
    "    def get_reaching_dir(angle):\n",
    "        \n",
    "        if 0 <= angle < 45:\n",
    "            return 0\n",
    "        elif 45 <= angle < 90:\n",
    "            return 1\n",
    "        elif 90 <= angle < 135:\n",
    "            return 2\n",
    "        elif 135 <= angle < 180:\n",
    "            return 3\n",
    "        elif 180 <= angle < 225:\n",
    "            return 4\n",
    "        elif 225 <= angle < 270:\n",
    "            return 5\n",
    "        elif 270 <= angle < 315:\n",
    "            return 6\n",
    "        elif 315 <= angle <= 360:\n",
    "            return 7\n",
    "        else:\n",
    "            raise ValueError(\"Angle out of range\")\n",
    "\n",
    "    return torch.tensor([get_reaching_dir(angle) for angle in angles])\n",
    "\n",
    "\n",
    "directions = [0, 1, 2]\n",
    "reach_dirs = get_reaching_dirs(reach_angles_deg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_psth(rates, reach_dirs=reach_dirs, directions=reach_dirs):\n",
    "    psth = np.zeros((len(directions), rates.shape[1], rates.shape[2]))\n",
    "\n",
    "    for i, direction in enumerate(directions):\n",
    "        mask = reach_dirs == direction\n",
    "        # We devide by the bin size (in sec) to get the firing rate as the number of spikes per second, not per bin.\n",
    "        psth[i, :, :] = rates[mask, :, :].mean(axis=0)\n",
    "    \n",
    "    return psth\n",
    "\n",
    "\n",
    "def calc_std_error(model_rates, directions=directions, reach_dirs=reach_dirs):\n",
    "    \n",
    "    std_error = []\n",
    "    for dir_idx, direction in enumerate(directions):\n",
    "        mask = reach_dirs == direction\n",
    "        \n",
    "        model_std = np.std(np.array(model_rates[mask, :, :]), axis=0)\n",
    "        sample_size = np.array(model_rates[mask, :, :]).shape[0]\n",
    "        \n",
    "        std_error = model_std / np.sqrt(sample_size)\n",
    "        bin_avg_std_error = np.mean(std_error, axis=1)\n",
    "            \n",
    "        std_error.append(bin_avg_std_error)\n",
    "    return np.array(std_error)\n",
    "\n",
    "\n",
    "def sample_psths(n_samples, directions):\n",
    "    \n",
    "    # From each sample of the inferred spikind data, calc the model_psth and put it here.\n",
    "    sampled_psths = []\n",
    "    \n",
    "    # Now, obs data -> XFADS -> inferred spikes -> PSTH\n",
    "    # Repeat fo a number of samples. Then use these samples to measure the mean ans std,\n",
    "    # i.e. how much the inferred PSTH dluctuates arount the mean PSTH.\n",
    "    for sample_i in range(n_samples):\n",
    "\n",
    "        z_train = [seq_vae.ssm(batch[0], cfg.n_samples)[1] for batch in train_dataloader]\n",
    "        z_valid = [seq_vae.ssm(batch[0], cfg.n_samples)[1] for batch in valid_dataloader]\n",
    "        z_test = [seq_vae.ssm(batch[0], cfg.n_samples)[1] for batch in test_dataloader]\n",
    "\n",
    "        spikes_trian_s = torch.poisson(\n",
    "            cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_train, dim=1))).mean(dim=0)\n",
    "        )\n",
    "\n",
    "        spikes_valid_s = torch.poisson(\n",
    "            cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_valid, dim=1))).mean(dim=0)\n",
    "        )\n",
    "\n",
    "        spikes_test_s = torch.poisson(\n",
    "            cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_test, dim=1))).mean(dim=0)\n",
    "        )\n",
    "\n",
    "        model_spikes = torch.concat([spikes_trian_s, spikes_valid_s, spikes_test_s], dim=0)\n",
    "        print(f'Reconstructed trilas sample: {sample_i+1}')\n",
    "        \n",
    "        # Push each of these samples through the velocity decoder.\n",
    "        clf = Ridge(alpha=0.01)\n",
    "\n",
    "        # Fit the velocity decoder on the inferred ratedby XFADS.\n",
    "        clf.fit(\n",
    "            torch.exp(\n",
    "                seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_train, dim=1))\n",
    "            ).mean(dim=0).reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2)\n",
    "        )\n",
    "\n",
    "        r2_train = clf.score(\n",
    "            torch.exp(\n",
    "                seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_train, dim=1))).mean(dim=0).reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2)\n",
    "        )\n",
    "\n",
    "        r2_valid = clf.score(\n",
    "            torch.exp(\n",
    "                seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_valid, dim=1))).mean(dim=0).reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2)\n",
    "        )\n",
    "\n",
    "        r2_test = clf.score(\n",
    "            torch.exp(\n",
    "                seq_vae.ssm.likelihood_pdf.readout_fn(torch.cat(z_test, dim=1))).mean(dim=0).reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2)\n",
    "        )\n",
    "\n",
    "        # Predict the hand velocity whilein these inferred spiking data,\n",
    "        # and convert it to x, y hand position cordinated, in each time bin.\n",
    "        vel_to_pos = lambda v: torch.cumsum(torch.tensor(v), dim=1)\n",
    "\n",
    "        pos_pred = clf.predict(model_spikes.reshape(-1, n_neurons_obs))\n",
    "                \n",
    "        pos_train_s = torch.tensor(pos_pred[:n_trials*cfg.n_bins].reshape(list(batch_sz_train) + [2]))\n",
    "        pos_valid_s = torch.tensor(pos_pred[n_trials*cfg.n_bins:(n_trials+n_trials_valid)*cfg.n_bins].reshape(list(batch_sz_valid) + [2]))\n",
    "        pos_test_s = torch.tensor(pos_pred[-n_trials_test*cfg.n_bins:].reshape(list(batch_sz_test) + [2]))\n",
    "\n",
    "        pos_s = vel_to_pos(\n",
    "            torch.concat([pos_train_s, pos_valid_s, pos_test_s], dim=0)\n",
    "        )\n",
    "\n",
    "        reach_angles_rad = torch.atan2(pos_s[:, -1, 0], pos_s[:, -1, 1])\n",
    "        reach_angles_deg = torch.rad2deg(reach_angles_rad) + 360 / 2\n",
    "\n",
    "        reach_dirs = get_reaching_dirs(reach_angles_deg)\n",
    "        \n",
    "        sampled_psths.append(calc_psth(model_rates, reach_dirs))\n",
    "        print(f'PSTH sample: {sample_i+1}')\n",
    "                \n",
    "    return sampled_psths\n",
    "\n",
    "\n",
    "def calc_mod_idx(psth):\n",
    "    \"\"\"\n",
    "    returns the total modulation index on neurons, by summing for all directons, for all time bins.\n",
    "    \"\"\"\n",
    "    mi =  np.sum((psth.max(axis=0) - psth.min(axis=0)) / (psth.max(axis=0) + psth.min(axis=0)), axis=0)\n",
    "    \n",
    "    return(np.nan_to_num(mi, nan=0))\n",
    "\n",
    "\n",
    "def calc_var_to_mean_ratio(psth):\n",
    "    v_m_ratio =  np.sum((psth.var(axis=0)) / (psth.mean(axis=0)), axis=0)\n",
    "    \n",
    "    return(np.nan_to_num(v_m_ratio, nan=0))\n",
    "\n",
    "'''\n",
    "def calc_var_to_mean_ratio(rates):\n",
    "    psth = calc_psth(rates)\n",
    "    v_m_ratio =  np.sum((rates.numpy().var(axis=0)) / (rates.numpy().mean(axis=0)), axis=0)\n",
    "    \n",
    "    return(np.nan_to_num(v_m_ratio, nan=0))\n",
    "'''\n",
    "\n",
    "def calc_model_data_mse(psth):\n",
    "    rates_mean = rates.numpy().mean(axis=0)\n",
    "    mse = np.sum(np.sum((psth - rates_mean) ** 2, axis=0), axis=0)\n",
    "    \n",
    "    return(np.nan_to_num(mse, nan=0))\n",
    "\n",
    "\n",
    "def get_index_of_precentile(data, percentile):\n",
    "    sorted_indices = np.argsort(data)\n",
    "    data_sorted = data[sorted_indices]\n",
    "    \n",
    "    cumulative_sum = np.cumsum(data_sorted)\n",
    "    threshold = percentile * cumulative_sum[-1]\n",
    "    \n",
    "    sorted_index = np.searchsorted(cumulative_sum, threshold)\n",
    "    \n",
    "    return sorted_indices[sorted_index]\n",
    "\n",
    "\n",
    "def get_abs_coeff(clf):\n",
    "    coefficients = np.abs(clf.coef_)\n",
    "    \n",
    "    return np.mean(coefficients, axis=0)\n",
    "\n",
    "\n",
    "def evaluate_neuron_impact(X_train, X_test, y_train, y_test, feature_idx):\n",
    "\n",
    "    X_train_reduced = np.delete(X_train, feature_idx, axis=1)\n",
    "    X_test_reduced = np.delete(X_test, feature_idx, axis=1)\n",
    "\n",
    "    clf_reduced = Ridge(alpha=0.01)\n",
    "    clf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "    return clf_reduced.score(X_test_reduced, y_test)\n",
    "\n",
    "\n",
    "def get_impact_on_r2(train_rates, test_rates, train_vel, test_vel):\n",
    "    original_accuracy = clf.score(test_rates.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    impact = []\n",
    "\n",
    "    for idx in range(test_rates.shape[2]):\n",
    "        accuracy_without_feature = evaluate_neuron_impact(\n",
    "            train_rates.reshape(-1, train_rates.shape[2]), test_rates.reshape(-1, test_rates.shape[2]),\n",
    "            train_vel.reshape(-1, 2), test_vel.reshape(-1, 2),\n",
    "            idx\n",
    "        )\n",
    "        impact.append(original_accuracy - accuracy_without_feature)\n",
    "        \n",
    "    return np.array(impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_psth = calc_psth(data_rates, reach_dirs, directions)[directions]\n",
    "model_psth = calc_psth(model_rates, reach_dirs, directions)[directions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_neurons_to_plot = 8\n",
    "neuron_indcs = np.random.choice(range(0, y_valid_obs.shape[2]), size=n_neurons_to_plot, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=n_neurons_to_plot, figsize=(n_neurons_to_plot * 1, n_neurons_to_plot * 1.5), sharex=True)\n",
    "fig.suptitle(f'Condition-averaged PSTH\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "for neuron_dx, neuron in enumerate(neuron_indcs):\n",
    "    for dir_idx, direction in enumerate(directions):\n",
    "\n",
    "        axes[0, 0].set_title('data\\n\\n\\n\\n\\n', fontsize=10)\n",
    "        axes[neuron_indcs.tolist().index(neuron), 0].plot(\n",
    "            np.arange(n_bins) * cfg.bin_sz_ms,\n",
    "            data_psth[dir_idx, :, neuron],\n",
    "            label=f'direction {direction}' if neuron == neuron_indcs[-1] else '')\n",
    "\n",
    "        axes[0, 1].set_title('model\\n\\n\\n\\n\\n', fontsize=10)\n",
    "        axes[neuron_indcs.tolist().index(neuron), 1].plot(\n",
    "            np.arange(n_bins) * cfg.bin_sz_ms,\n",
    "            model_psth[dir_idx, :, neuron]) \n",
    "        axes[n_neurons_to_plot-1, 0].set_xlabel('\\ntime (ms)' if neuron == neuron_indcs[-1] else None, fontsize=9)\n",
    "        axes[neuron_indcs.tolist().index(neuron), 0].set_ylabel(f'neuron {neuron+1} \\n', fontsize=9)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(axis='x', labelsize=8)\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        ax.axvline(x=b_test[:, 0].mean(dim=0) * cfg.bin_sz_ms, color='coral', linestyle='--')\n",
    "        ax.axvline(x=b_test[:, 1].mean(dim=0) * cfg.bin_sz_ms, color='navy', linestyle='--')\n",
    "        ax.axvline(x=b_test[:, 2].mean(dim=0) * cfg.bin_sz_ms, color='gold', linestyle='--')\n",
    "\n",
    "        if neuron == neuron_indcs[0]:\n",
    "            mark_events_on_psth(ax, cfg, b_test)\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=1, fontsize=8)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('output_figs/cond_avg_psth.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(reach_dirs, bins=range(len(directions)+1), align='left', rwidth=0.8)\n",
    "plt.xlabel('reaching direction')\n",
    "plt.ylabel('num of trials')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig('output_figs/conds_counts.png')\n",
    "\n",
    "for count, bin_edge in zip(n, bins):\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    bin_center = bin_edge\n",
    "    plt.text(\n",
    "        bin_center, count+10, f'{int(count)}', ha='center', va='bottom', fontsize=10, color='black'\n",
    "    )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# r2 for each of the reaching directions\n",
    "dirs_r2 = np.zeros((len(directions)))\n",
    "# The stds of the r2s of all trials in each of the directions\n",
    "dirs_trials_r2_std = np.zeros((len(directions)))\n",
    "# List of arrays of the r2 of each trial in each direction\n",
    "dirs_trials_r2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dir_idx, direction in enumerate(directions):\n",
    "\n",
    "        test_reach_dirs = get_reaching_dirs(\n",
    "            (torch.rad2deg(torch.atan2(pos_test[:, -1, 0], pos_test[:, -1, 1])) + 360 / 2)\n",
    "        )\n",
    "        \n",
    "        # A mask to get the trials for direction dir_idx\n",
    "        mask = test_reach_dirs == direction\n",
    "        \n",
    "        # Calculate the r2 for just direction dir_idx\n",
    "        dir_r2 = clf.score(rates_test_s[mask, :, :].reshape(-1, n_neurons_obs) + 1, vel_test[mask, :, :].reshape(-1, 2) + 1)\n",
    "        dirs_r2[dir_idx] = dir_r2\n",
    "        \n",
    "        dirs_trials_r2.append(\n",
    "            np.array(\n",
    "                [clf.score(rates_test_s[mask, :, :][trial_idx], vel_test[mask, :, :][trial_idx]) for trial_idx in range(rates_test_s[mask, :, :].shape[0])]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Get the std of the r2 of the trials in direction dir_idx\n",
    "        dir_trials_r2_std = np.std(\n",
    "            [clf.score(rates_test_s[mask, :, :][trial_idx], vel_test[mask, :, :][trial_idx]) for trial_idx in range(rates_test_s[mask, :, :].shape[0])]\n",
    "        )\n",
    "        dirs_trials_r2_std[dir_idx] = dir_trials_r2_std\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=1, figsize=(6, 6), sharex=True)\n",
    "fig.suptitle(f'test data r2', fontsize=12)\n",
    "\n",
    "colors = sns.color_palette(\"coolwarm\", len(directions)).as_hex()\n",
    "\n",
    "axes.bar(directions, dirs_r2, yerr=dirs_trials_r2_std, color=colors)\n",
    "\n",
    "axes.set_xlabel('direction')\n",
    "axes.set_ylabel('r2')\n",
    "\n",
    "axes.tick_params(axis='x', labelsize=10)\n",
    "axes.tick_params(axis='y', labelsize=10)\n",
    "axes.spines['top'].set_visible(False)\n",
    "axes.spines['right'].set_visible(False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('output_figs/r2_std.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# r2 for each of the reaching directions\n",
    "dirs_r2 = np.zeros((len(directions)))\n",
    "# The stds of the r2s of all trials in each of the directions\n",
    "dirs_trials_r2_std = np.zeros((len(directions)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dir_idx, direction in enumerate(directions):\n",
    "        \n",
    "        pos = torch.concat([pos_train, pos_valid, pos_test], dim=0)\n",
    "        reach_dirs = get_reaching_dirs(\n",
    "            (torch.rad2deg(torch.atan2(pos[:, -1, 0], pos[:, -1, 1])) + 360 / 2)\n",
    "        )\n",
    "\n",
    "        # A mask to get the trials for direction dir_idx\n",
    "        mask = reach_dirs == direction\n",
    "        \n",
    "        # Calculate the r2 for just direction dir_idx\n",
    "        dir_r2 = clf.score(model_rates[mask, :, :].reshape(-1, n_neurons_obs), vel[mask, :, :].reshape(-1, 2))\n",
    "        dirs_r2[dir_idx] = dir_r2\n",
    "        \n",
    "        # Get the std of the r2 of thee trials in direction dir_idx\n",
    "        dir_trials_r2_std = np.std(\n",
    "            [clf.score(model_rates[mask, :, :][trial_idx], vel[mask, :, :][trial_idx]) for trial_idx in range(model_rates[mask, :, :].shape[0])]\n",
    "        )\n",
    "        dirs_trials_r2_std[dir_idx] = dir_trials_r2_std\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=1, figsize=(6, 6), sharex=True)\n",
    "fig.suptitle(f'data r2', fontsize=12)\n",
    "\n",
    "colors = sns.color_palette(\"coolwarm\", len(directions)).as_hex()\n",
    "\n",
    "axes.bar(directions, dirs_r2, yerr=dirs_trials_r2_std, color=colors)\n",
    "\n",
    "axes.set_xlabel('direction')\n",
    "axes.set_ylabel('r2')\n",
    "\n",
    "axes.spines['top'].set_visible(False)\n",
    "axes.spines['right'].set_visible(False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('output_figs/r2_std_all_trials.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_psth(data_psth, model_psth, bhv_bins, clf=clf, directions=directions, n_neurons_to_plot=8, orderby=None):\n",
    "    \"\"\"\n",
    "    Condition-averaged PSTH of the top <n_neurons_to_plot> neurons ordered by <orderby>.\n",
    "    \"\"\"\n",
    "        \n",
    "    if orderby == 'fr':\n",
    "        avg_fr = data_rates.numpy().mean(axis=0).mean(axis=0)\n",
    "        #neuron_indcs = np.array([np.argsort(avg_fr)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(avg_fr, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by trial-averaged firing rate)\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_fr.png'\n",
    "\n",
    "    elif orderby == 'mi':\n",
    "        mi = calc_mod_idx(data_psth)\n",
    "        #neuron_indcs = np.array([np.argsort(mi)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(mi, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by tuning depth measured by modulation index)\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_mi.png'\n",
    "\n",
    "    elif orderby == 'vmr':\n",
    "        vmr = calc_var_to_mean_ratio(data_psth)\n",
    "        #neuron_indcs = np.array([np.argsort(vmr)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(vmr, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by tuning depth measured by variance-to-mean ratio)\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_vmr.png'\n",
    "\n",
    "    elif orderby == 'mse':\n",
    "        mse = calc_model_data_mse(data_rates)\n",
    "        #neuron_indcs = np.array([np.argsort(mse)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(mse, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by tuning depth measured by MSE)\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_mse.png'\n",
    "\n",
    "    elif orderby == 'r2':\n",
    "        r2 = get_impact_on_r2(rates_train_s, rates_test_s, vel_train, vel_test)\n",
    "        #neuron_indcs = np.array([np.argsort(r2)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(r2, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by impact on velocity decoding measured by reduction in accuracy due to dropping each neuron)\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_r2.png'\n",
    "\n",
    "    elif orderby == 'coeff':\n",
    "        coeffs = get_abs_coeff(clf)\n",
    "        #neuron_indcs = np.array([np.argsort(coeffs)[-i] for i in range(1, n_neurons_to_plot+1)])\n",
    "        neuron_indcs = np.array([get_index_of_precentile(coeffs, precentile) for precentile in np.linspace(1, 0, n_neurons_to_plot)])\n",
    "        title = 'Condition-averaged PSTH\\n(neurons sorted by impact on velocity decoding measured by the abs value of coefficients\\nin the decoder)\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth_orderedby_coeff.png'\n",
    "\n",
    "    elif orderby is None:\n",
    "        neuron_indcs = np.random.choice(range(0, y_valid_obs.shape[2]), size=n_neurons_to_plot, replace=False)\n",
    "        title = 'Condition-averaged PSTH\\n\\n\\n\\n'\n",
    "        filename = 'cond_avg_psth.png'\n",
    "\n",
    "    else:\n",
    "        print(\"Valid options for ordering neurons: 'fr', 'mi', 'vmr', 'r2', 'mse', 'coeff', None\")\n",
    "        pass\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(directions), nrows=n_neurons_to_plot, figsize=(12, 12), sharex=True)\n",
    "    fig.suptitle(title)\n",
    "    colors = sns.color_palette(\"viridis\", len(directions)).as_hex()\n",
    "\n",
    "    for neuron_idx, neuron in enumerate(neuron_indcs):\n",
    "        for dir_idx, direction in enumerate(directions):\n",
    "\n",
    "            std_error = np.std((model_rates[reach_dirs == direction][:, :, neuron]).numpy(), axis=0) / np.sqrt(model_rates[reach_dirs == direction][:, :, neuron].shape[0])\n",
    "\n",
    "            axes[0, dir_idx].set_title(f'\\ndirection {dir_idx}\\n\\n\\n\\n\\n', fontsize=8)\n",
    "\n",
    "            axes[neuron_idx, dir_idx].plot(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                data_psth[dir_idx, :, neuron],\n",
    "                #color=colors[dir_idx],\n",
    "                color='black',\n",
    "                alpha=1.0,\n",
    "                linewidth=0.8,\n",
    "                label=f'data' if neuron == neuron_indcs[-1] and direction == directions[-1] else ''\n",
    "            )\n",
    "\n",
    "            axes[neuron_idx, dir_idx].plot(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                model_psth[dir_idx, :, neuron],\n",
    "                #color=colors[dir_idx],\n",
    "                color='magenta',\n",
    "                linewidth=0.6,\n",
    "                alpha=1.0,\n",
    "                label=f'model' if neuron == neuron_indcs[-1] and direction == directions[-1] else ''\n",
    "            )\n",
    "\n",
    "            axes[neuron_idx, dir_idx].fill_between(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                model_psth[dir_idx, :, neuron] - std_error,\n",
    "                model_psth[dir_idx, :, neuron] + std_error,\n",
    "                color='pink',\n",
    "                alpha=0.5,\n",
    "                label='model std error' if neuron == neuron_indcs[-1] and dir_idx == len(directions)-1 else ''\n",
    "            )\n",
    "            \n",
    "            #if neuron_idx == 0 and dir_idx == 0:\n",
    "            #    mark_events_on_psth(axes[neuron_idx, dir_idx], cfg, bhv_bins)\n",
    "\n",
    "            axes[n_neurons_to_plot-1, 0].set_xlabel('time (ms)' if neuron == neuron_indcs[-1] else None, fontsize=6)\n",
    "            axes[neuron_indcs.tolist().index(neuron), 0].set_ylabel(f'neuron {neuron+1}', fontsize=6)\n",
    "\n",
    "            ymax = max(np.max(data_psth[:, :, neuron].flatten()), np.max(model_psth[:, :, neuron].flatten())) * 1.1\n",
    "            axes[neuron_idx, dir_idx].set_ylim(0, ymax)\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.tick_params(axis='x', labelsize=6)\n",
    "            ax.tick_params(axis='y', labelsize=6)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "            ax.axvline(x=bhv_bins[:, 0].mean(dim=0) * cfg.bin_sz_ms, color='coral', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 1].mean(dim=0) * cfg.bin_sz_ms, color='navy', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 2].mean(dim=0) * cfg.bin_sz_ms, color='gold', linestyle='--', linewidth=0.5)\n",
    "            \n",
    "    mark_events_on_psth(axes[0, 0], cfg, bhv_bins)\n",
    "    \n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.92), ncol=1, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f'output_figs/data_model_{filename}')\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(directions), nrows=n_neurons_to_plot, figsize=(12,  12), sharex=True)\n",
    "    fig.suptitle(title)\n",
    "    colors = sns.color_palette(\"viridis\", len(directions)).as_hex()\n",
    "\n",
    "    for neuron_idx, neuron in enumerate(neuron_indcs):\n",
    "        for dir_idx, direction in enumerate(directions):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                std_error = np.std((model_rates[reach_dirs == direction][:, :, neuron]).numpy(), axis=0) / np.sqrt(model_rates[reach_dirs == direction][:, :, neuron].shape[0])\n",
    "\n",
    "            axes[0, dir_idx].set_title(f'\\ndirection {dir_idx}\\n\\n\\n\\n\\n', fontsize=8)\n",
    "\n",
    "            axes[neuron_idx, dir_idx].plot(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                model_psth[dir_idx, :, neuron],\n",
    "                #color=colors[dir_idx],\n",
    "                color='purple',\n",
    "                linewidth=1.0,\n",
    "                alpha=0.8,\n",
    "                label=f'model' if neuron == neuron_indcs[-1] and direction == directions[-1] else ''\n",
    "            )\n",
    "\n",
    "            axes[neuron_idx, dir_idx].fill_between(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                model_psth[dir_idx, :, neuron] - std_error,\n",
    "                model_psth[dir_idx, :, neuron] + std_error,\n",
    "                color='pink',\n",
    "                alpha=0.5,\n",
    "                label='std error' if neuron == neuron_indcs[-1] and dir_idx == len(directions)-1 else ''\n",
    "            )\n",
    "            \n",
    "            #if neuron_idx == 0 and dir_idx == 0:\n",
    "            #    mark_events_on_psth(axes[neuron_idx, dir_idx], cfg, bhv_bins)\n",
    "\n",
    "            axes[n_neurons_to_plot-1, 0].set_xlabel('time (ms)' if neuron == neuron_indcs[-1] else None, fontsize=6)\n",
    "            axes[neuron_indcs.tolist().index(neuron), 0].set_ylabel(f'neuron {neuron+1}', fontsize=6)\n",
    "\n",
    "            ymax = np.max(model_psth[:, :, neuron].flatten()) * 1.1\n",
    "            axes[neuron_idx, dir_idx].set_ylim(0, ymax)\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.tick_params(axis='x', labelsize=6)\n",
    "            ax.tick_params(axis='y', labelsize=6)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "            ax.axvline(x=bhv_bins[:, 0].mean(dim=0) * cfg.bin_sz_ms, color='coral', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 1].mean(dim=0) * cfg.bin_sz_ms, color='navy', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 2].mean(dim=0) * cfg.bin_sz_ms, color='gold', linestyle='--', linewidth=0.5)\n",
    "            \n",
    "    mark_events_on_psth(axes[0, 0], cfg, bhv_bins)\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.92), ncol=1, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f'output_figs/model_{filename}')\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(directions), nrows=n_neurons_to_plot, figsize=(12,  12), sharex=True)\n",
    "    fig.suptitle(title)\n",
    "    colors = sns.color_palette(\"viridis\", len(directions)).as_hex()\n",
    "\n",
    "    for neuron_idx, neuron in enumerate(neuron_indcs):\n",
    "        for dir_idx, direction in enumerate(directions):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                std = np.std((data_rates[reach_dirs == direction][:, :, neuron]).numpy(), axis=0)\n",
    "\n",
    "            axes[0, dir_idx].set_title(f'\\ndirection {dir_idx}\\n\\n\\n\\n\\n\\n', fontsize=8)\n",
    "\n",
    "            axes[neuron_idx, dir_idx].plot(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                data_psth[dir_idx, :, neuron],\n",
    "                #color=colors[dir_idx],\n",
    "                color='green',\n",
    "                linewidth=1.0,\n",
    "                alpha=0.8,\n",
    "                label=f'data' if neuron == neuron_indcs[-1] and direction == directions[-1] else ''\n",
    "            )\n",
    "\n",
    "            axes[neuron_idx, dir_idx].fill_between(\n",
    "                np.arange(cfg.n_bins) * cfg.bin_sz_ms,\n",
    "                data_psth[dir_idx, :, neuron] - std/2,\n",
    "                data_psth[dir_idx, :, neuron] + std/2,\n",
    "                color='skyblue',\n",
    "                alpha=0.5,\n",
    "                label='std' if neuron == neuron_indcs[-1] and dir_idx == len(directions)-1 else ''\n",
    "            )\n",
    "            \n",
    "            #if neuron_idx == 0 and dir_idx == 0:\n",
    "            #    mark_events_on_psth(axes[neuron_idx, dir_idx], cfg, bhv_bins)\n",
    "\n",
    "            axes[n_neurons_to_plot-1, 0].set_xlabel('time (ms)' if neuron == neuron_indcs[-1] else None, fontsize=6)\n",
    "            axes[neuron_indcs.tolist().index(neuron), 0].set_ylabel(f'neuron {neuron+1}', fontsize=6)\n",
    "\n",
    "            ymax = np.max(data_psth[:, :, neuron].flatten()) * 1.1\n",
    "            axes[neuron_idx, dir_idx].set_ylim(0, ymax)\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.tick_params(axis='x', labelsize=6)\n",
    "            ax.tick_params(axis='y', labelsize=6)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "            ax.axvline(x=bhv_bins[:, 0].mean(dim=0) * cfg.bin_sz_ms, color='coral', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 1].mean(dim=0) * cfg.bin_sz_ms, color='navy', linestyle='--', linewidth=0.5)\n",
    "            ax.axvline(x=bhv_bins[:, 2].mean(dim=0) * cfg.bin_sz_ms, color='gold', linestyle='--', linewidth=0.5)\n",
    "            \n",
    "    mark_events_on_psth(axes[0, 0], cfg, bhv_bins)\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.92), ncol=1, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f'output_figs/data_{filename}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_psth(data_psth, model_psth, b_test, orderby=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_psth(data_psth, model_psth, b_test, orderby='vmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_psth(data_psth, model_psth, b_test, orderby='coeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotask",
   "language": "python",
   "name": "neurotask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
